모빌리티 빅데이터 실무
===

세부정보
---

- 담당 : 최혁두 교수님
- 일시 : 화  14:00 ~ 17:00
- 주 사용 플랫폼 : Flow, Zoom, github.io
- 주차 : 2주차 - 21.03.09

---

참조
---

Reference : [IanLecture][IanLecture_link] (교수님 블로그)

[IanLecture_link]: https://goodgodgd.github.io/ian-lecture/archivers/dt-classifier "IanLecture_Reference"

---



Image Classifiers
===

- 목적 : 오브젝트 디텍터(Object Detector)를 구현하기 위함



Implement Image Classifiers
==

영상 분류 모델을 생성, 학습, 평가 코드 구현을 텐서플로와 파이토치 두 가지 프레임워크에서 배워본다.



1. Tensorflow2
--



### 1.1. Tensor Operation (###)

텐서플로에서 모든 연산은 `tf.Tensor`라는 타입의 데이터로 진행된다. 
모든 파이썬 기반 딥러닝 프레임워크는 Numpy와 유사한 사용법을 가지고 있어 Numpy와 다차원 배열에 대한 개념이 있다면 크게 어렵지 않게 사용할 수 있다.



`tf.Tensor`의 자세한 사용방법은 tensorflow.org 홈페이지에 가이드에 잘 설명이 되어있다.
시작 전에 한번 다 읽어보는 것이 좋을 것 같다.

https://www.tensorflow.org/guide/tensor
이 홈페이지에 잘 설명되어 있다.



- `tf.Tensor`를 사용하기 위해 알아야 할 내용

#### 1. immutable (불변 객체)  (####)

> All tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.`
>
> > 모든 텐서는 파이썬 숫자와 문자열과 같이 불변합니다. 텐서의 내용은 업데이트할 수 없고 새 텐서만 생성할 수 있습니다.



`tf.Tensor`는 처음에 만들수만 있고 수정할 수 없다. 따라서 어떤 텐서 객체가 있을 때 내부의 상태를 바꿀 수 없기 때문에 그것을 수정한 새로운 텐서를 만들어야 한다.



#### 2. 생성 및 기본 속성 확인  (####)

```python
import tensorflow as tf

# create Tensor from List with specific type
tensor_a = tf.constant([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]], dtype=tf.int32)

print("Type of every element:", tensor_a.dtype)
print("Number of axes (=Rank):", tensor_a.ndim)
print("Shape of tensor:", tensor_a.shape)
print("Total number of elements: ", tf.size(tensor_a).numpy())
```

---

```python
Type of every element: <dtype: 'int32'>
Number of axes (=Rank): 3
Shape of tensor: (2, 2, 3)
Total number of elements:  tf.Tensor(12, shape=(), dtype=int32)
```



#### 3. Numpy 변환 (####)

`tf.Tensor`와 `np.array` 사이의 변환

```python
print("To numpy array:\n", x.numpy()[0])
print("Back to Tensor:", tf.convert_to_tensor(x.numpy())[0])
```

---

```python
To numpy array:
 [[1 2 3]
 [4 5 6]]
Back to Tensor: tf.Tensor(
[[1 2 3]
 [4 5 6]], shape=(2, 3), dtype=int32)
```



 1.2. Keras Classifier
--

텐서플로 내부의 Keras를 이용한 영상 분류 방법

- [텐서플로우 초보자 튜토리얼](https://www.tensorflow.org/tutorials/quickstart/beginner)
- [텐서플로우 케라스-sequential 모델 가이드](https://www.tensorflow.org/guide/keras/sequential_model)

Keras는 잘 만들어진 모듈들이 있어 이것들을 활용하면 간단한 분류 모델은 아주 짧은 코드로도 모델의 정의, 학습, 평가 까지 가능하다.



### a) 코드 구조

모든 프로그래밍을 할때 먼저 생각나는 것을 짜는 것이 아닌, 최상위 구조를 먼저 정하고 점차 하부구조를 정하면서 구체적인 알고리즘까지 작성하는 계층식으로 작성해야 한다.
그래야 오류도 적고 코드를 수정할 일이 적어진다.
최상위 구조는 다음의 코드와 같이 작성하며 대강의 상위 함수들과 입출력 데이터만 정하고 나서 함수들을 기계적으로 코딩하면 프로그램을 완성할 수 있다.

``` python 
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import pprint
from timeit import default_timer as timer

"""
Common utils
"""
class DurationTime:
    pass

def gpu_config():
    pass

def load_dataset(dataname="cifar10", show_imgs=True):
    pass

def show_samples(images, labels, class_names, grid=(3,4)):
    pass

"""
Classifier
"""
def tf2_keras_classifier():
    gpu_config()
    train_data, test_data = load_dataset()
    model = create_model(train_data)
    train_model(model, train_data)
    test_model(model, test_data)

def load_dataset():
    pass

def create_model(dataset):
    pass

def train_model(model, train_data, split_ratio=0.8):
	pass

def test_model(model, test_data):
	pass

if __name__ == "__main__":
    tf2_keras_classifier()
```

교수님이 테스트 하실때 Conv2D 레이어 사용시 다음과 같은 에러가 나서 프로그램 시작시 `gpu_config()`함수를 실행하였다고 한다.

> tensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked! [[node sequential/conv1/Conv2D (defined at /workspace/detlec/01_classifiers/tf_classifier_minimal.py:76) ]] [Op:__inference_train_function_760]

`gpu_config()` 함수내용은 학습하면서 사용하는 GPU 메모리를 확장할 수 있게 해준다.

```  python
def gpu_config():
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # Currently, memory growth needs to be the same across GPUs
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            logical_gpus = tf.config.experimental.list_logical_devices('GPU')
            print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
        except RuntimeError as e:
            # Memory growth must be set before GPUs have been initialized
            print(e)
```



### b) 데이터 불러오기

`tf.keras.datasets`에는 머신러닝에서 예제로 많이 사용되는 다양한 데이터셋을 자체 제공한다.
데이터 목록은 링크를 참고한다.

- https://www.tensorflow.org/api_docs/python/tf/keras/datasets
- https://www.cs.toronto.edu/~kriz/cifar.html

여기서는 CIFAR-10 데이터셋을 사용한다.

아래 `load_data` 함수에서는 일단 CIFAR-10 데이터 셋만 불러올 수 있도록 만들었지만 MNIST 같은 다른 데이터셋으로도 확장 가능하다.

``` python
def load_dataset(dataname="cifar10", show_imgs=True):
    if dataname == "cifar10":
        dataset = tf.keras.datasets.cifar10
        class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
    else:
        raise ValueError(f"Invalid dataset name: {dataname}")

    (x_train, y_train), (x_test, y_test) = dataset.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    y_train, y_test = y_train[:, 0], y_test[:, 0]
    print(f"Load {dataname} dataset:", x_train.shape, y_train.shape, x_test.shape, y_test.shape)
    if show_imgs:
        show_samples(x_train, y_train, class_names)
    return (x_train, y_train), (x_test, y_test)

def show_samples(images, labels, class_names, grid=(3,4)):
    plt.figure(figsize=grid)
    num_samples = grid[0] * grid[1]
    for i in range(num_samples):
        plt.subplot(grid[0], grid[1], i+1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(images[i])
        plt.xlabel(class_names[labels[i]])
    plt.show()
```



### C) 모델 정의 (Sequential Model)

텐서플로에서 모델을 정의하는 방법은 크게 두 가지가 있다.

1. `tf.keras.Sequential` 클레스를 사용하는 모델
2. 직접 텐서 연산의 흐름을 지정해주는 `Functional API`

Sequential 클래스를 사용하는 순차모델은 매우 간단하게 모델을 정의할 수 있지만, 일렬로 연결된 레이어로 이루어진 단순한 모델만 정의할 수 있다.
반면 Funtional API를 사용하면 코드가 조금 늘어나지만 제약없이 원하는 형태의 모델을 정의할 수 있다.
텐서플로에서 다양한 Layer 클래스를 제공하므로 Functional API도 그렇게 어렵지는 않다고 한다.

간편한 순차 모델을 구현해보면

``` python
def create_model(dataset, use_add=True):
    x, y = dataset
    input_shape = tf.shape(x)[1:].numpy()
    num_class = tf.reduce_max(y).numpy() + 1
    print(f"[create_model] input shape={input_shape}, num_class={num_class}")

    if use_add:
        model = keras.Sequential(name="tf-classifier")
        model.add(layers.Input(shape=input_shape))
        model.add(layers.Conv2D(filters=32, kernel_size=3, padding="same", activation="relu", name="conv1"))
        model.add(layers.MaxPool2D(pool_size=(2, 2), name="pooling1"))
        model.add(layers.Conv2D(filters=64, kernel_size=3, padding="same", activation="relu", name="conv2"))
        model.add(layers.MaxPool2D(pool_size=(2, 2), name="pooling2"))
        model.add(layers.Flatten(name="flatten"))
        model.add(layers.Dense(units=100, activation="relu", name="dense1"))
        model.add(keras.layers.Dropout(0.2))
        model.add(layers.Dense(units=num_class, activation="softmax", name="dense2"))
    else:
        model = keras.Sequential([
            layers.Conv2D(filters=32, kernel_size=3, padding="same", activation="relu", input_shape=input_shape, name="conv1"),
            layers.MaxPool2D(pool_size=(2, 2), name="pooling1"),
            layers.Conv2D(filters=64, kernel_size=3, padding="same", activation="relu", name="conv2"),
            layers.MaxPool2D(pool_size=(2, 2), name="pooling2"),
            layers.Flatten(name="flatten"),
            layers.Dense(units=100, activation="relu", name="dense1"),
            keras.layers.Dropout(0.2),
            layers.Dense(units=num_class, activation="softmax", name="dense2"),
            ],
            name="tf-classifier")
    model.summary()
    keras.utils.plot_model(model, "tf-clsf-model.png")
    return model
```

**순차 모델**을 정의하는 방법도 두 가지가 있다.
위의 함수에서는 `use_add`옵션을 통해 둘 중 하나를 고를 수 있게 했다.

- `model.add()` : 함수를 통해 레이어를 하나씩 추가하기.
- `keras.Sequential` : 클래스 생성자에 레이어 객체들을 `list`로 한번에 넣기.

모델을 정의할 때 입력 shape을 정할수도 있고 않을수도 있는데, 정하지 않으면 모델 내부의 텐서 크기가 결정되지 않으므로 밑에서 `model.summary()`에서 에러가 난다. 
**입력 shape** 를 지정하는 방법도 두가지가 있다.

- `layers.Input`을 첫 번째 레이어로 추가.
- 첫 번째 연산 레이어에서 `input_shape`옵션을 지정

레이어의  activation은 `tf.keras.activations`아래의 클래스 객체를 입력해도 되고 각 클래스를 나타내는 물자열을 입력해도 된다.
입력가능한 activation 종류는 다음 링크에서 참조할 수 있다.
https://keras.io/api/layers/activations/
activation의 기본 값은 "linear"로 출력된 값을 그대로 내보내는 것이다.

`model.summary()`를 실행하면 현재 정의된 모델의 구조를 깔끔하게 터미널에서 보여준다.
실행결과는 다음과 같다.

``` shell
Model: "tf-classifier"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1 (Conv2D)               (None, 32, 32, 32)        896       
_________________________________________________________________
pooling1 (MaxPooling2D)      (None, 16, 16, 32)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 16, 16, 64)        18496     
_________________________________________________________________
pooling2 (MaxPooling2D)      (None, 8, 8, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 4096)              0         
_________________________________________________________________
dense1 (Dense)               (None, 100)               409700    
_________________________________________________________________
dropout (Dropout)            (None, 100)               0         
_________________________________________________________________
dense2 (Dense)               (None, 10)                1010      
=================================================================
Total params: 430,102
Trainable params: 430,102
Non-trainable params: 0
_________________________________________________________________
```

`keras.utils.plot_model`함수는 모델의 그래프 구조를 그림으로 그려준다.
실행결과는 다음과 같다.

<img src="E:\md\수업자료\빅데이터실무\2week\tf-clsf-model.png" width="30%" height="20%" title="px(픽셀) 크기설정" alt="모델_비교"></img>



### d) 학습

Keras에서 분류 모델은 `model.fit()`함수로 간단히 학습이 가능하다.
그 전에 학습 데이터를 실제 학습용과 검증(validation)용으로 나누어 학습중에 overfitting이 일어나는지 확인한다.
모델을 학습시킬 loss와 optimizer는 `model.compile()`함수에서 미리 입력한다.

``` python
def train_model(model, train_data, split_ratio=0.8):
    x, y = train_data
    trainlen = x.shape[0] * split_ratio
    x_train, y_train = x[:trainlen], y[:trainlen]
    x_val, y_val = x[trainlen:], y[trainlen:]

    model.compile(
        loss=keras.losses.SparseCategoricalCrossentropy(),
        optimizer=keras.optimizers.RMSprop(),
        metrics=[keras.metrics.SparseCategoricalAccuracy()],
    )

    with DurationTime("** training time") as duration:
        history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))
    history = {key: np.array(values) for key, values in history.history.items()}
    np.set_printoptions(precision=4, suppress=True)
    pp = pprint.PrettyPrinter(indent=2, width=100, compact=True)
    print("[train_model] training history:")
    pp.pprint(history)
```

학습을 하고 나면 history 객체가 나오는데 `history.history`에는 학습하면서 epoch마다 계산한 평균 loss와 metric(accuracy)이 저장되어 있다.
실행결과는 다음과 같다.
결과를 보면 학습이 진행됨에 따라 학습 데이터와 검증 데이터에서 모두 loss는 줄어들고 accuracy는 늘어나는 것을 볼 수 있다.

``` python
Epoch 1/5
1250/1250 [==============================] - 4s 2ms/step - loss: 1.7264 - sparse_categorical_accuracy: 0.3778 - val_loss: 1.1578 - val_sparse_categorical_accuracy: 0.5921
Epoch 2/5
1250/1250 [==============================] - 2s 2ms/step - loss: 1.1409 - sparse_categorical_accuracy: 0.5991 - val_loss: 0.9853 - val_sparse_categorical_accuracy: 0.6565
Epoch 3/5
1250/1250 [==============================] - 2s 2ms/step - loss: 0.9722 - sparse_categorical_accuracy: 0.6597 - val_loss: 1.0722 - val_sparse_categorical_accuracy: 0.6253
Epoch 4/5
1250/1250 [==============================] - 2s 2ms/step - loss: 0.8693 - sparse_categorical_accuracy: 0.6976 - val_loss: 0.9583 - val_sparse_categorical_accuracy: 0.6853
Epoch 5/5
1250/1250 [==============================] - 2s 2ms/step - loss: 0.7831 - sparse_categorical_accuracy: 0.7288 - val_loss: 0.9265 - val_sparse_categorical_accuracy: 0.6882
** training time: 13.19
[train_model] training history:
{ 'loss': array([1.5012, 1.1168, 0.9691, 0.8737, 0.8066]),
  'sparse_categorical_accuracy': array([0.4628, 0.608 , 0.6622, 0.6961, 0.7221]),
  'val_loss': array([1.1578, 0.9853, 1.0722, 0.9583, 0.9265]),
  'val_sparse_categorical_accuracy': array([0.5921, 0.6565, 0.6253, 0.6853, 0.6882])}
```



#### `Context Manager`

`train_model()`함수에서는 학습에 걸린 시간을 context manager를 이용하여 측정한다.
Context manager를 이용하면 `with`블럭의 시작과 종료시에 할 일을 정의할 수 있다.
주로 파일 입출력을 할 때 처럼 시작 할 때 자원을 할당하고 나갈때 해제하는 용도로 쓰이지만 필요에 따라 다양하게 응용할 수 있다.

Context manager는 주로 클래스로 구현되며, 다음 두 개의 매직 메소드(magic method)를 반드시 구형해야 한다.

- `__enter__()` : `with`블럭을 들어갈 때 실행되고 함수의 리턴 값이 `as`뒤에 붙는 변수로 들어간다.
- `__exit__(type, value, trace_back)` : `with` 블럭을 나가거나 내부에서 예외 발생 시 실행된다.
  주로 할당한 자원을 해제하거나 예외처리 기능을 한다.

Context manager 예제는 다음 링크에서 볼 수 있다.
https://planbs.tistory.com/entry/Python-Context-Manager

여기서는 `DurationTime`이라는 Context manager를 만들어서 학습에 걸린 시간을 측정했는데 관련 코드는 다음과 같다.

``` python
class DurationTime:
    def __init__(self, context):
        self.start = 0
        self.context = context

    def __enter__(self):        # entering 'with' context
        self.start = timer()
        return self             # pass object by 'as'

    def __exit__(self, type, value, trace_back):    # exiting 'with' context
        print(f"{self.context}: {timer() - self.start:1.2f}")

def train_model(model, train_data, split_ratio=0.8):
    ...
    with DurationTime("** training time") as duration:
        history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))
```

만약 Context manager를 사용하지 않았다면 이렇게 시간을 측정했을 것이다.

```python
def train_model(model, train_data, split_ratio=0.8):
    ...
    start = timer()
    history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))
    print(f"** trainig time: {timer() - start:1.2f}")
```

이게 코드가 덜 들어가는 것처럼 보이지만, 시간을 측정해야 하는 상황이 여러번 생긴다면 측정해야 하는 라인들 앞뒤로 짝을 맞춰서 두 줄씩 코딩을 해야 한다.

#### 짝 맞춰 코딩

```python
def some_func():
    start_total = timer()
    start = timer()
    sub_func1()
    print(f"elapsed time1:", timer() - start)
    start = timer()
    sub_func1()
    print(f"elapsed time2:", timer() - start)
    print(f"total elapsed time:", timer() - start_total)
```

#### Context manager 사용

```python
def some_func():
    with DurationTime("total elapsed time"):
        with DurationTime("elased time1"):
            sub_func1()
        with DurationTime("elased time1"):
            sub_func1()
```

어느 것이 보기에도 편하고 이해가 잘될까
프로그램의 중요 흐름에 상관 없는 불필요한 코드가 많아지면 보기에도 좋지않고 코드의 가독성이 떨어진다,.
그리고 이렇게 짝을 맞춰서 코딩하는 것은 실수를 유발하기 때문에 좋지 않다.

C++에서 `new`와 `delete`짝을 맞워야 하는 문제 때문에 스마트 포인터가 나왔다는 것을 기억하자.
Context manager를 이용하면 들여쓰기로 인해 context의 범의가 명확히 보이고 코드가 입체적으로 구조화되어 가독성이 좋아진다.



### e) 평가

Keras에서 분류 모델의 정확도는 `model.evaluate()`함수로 계산할 수 있다.
여기서는 `model.predict()`도 실행하여 분류 결과를 확인하고 직접 분류 정확도까지 계산해보았다.

``` python
def test_model(model, test_data):
    x_test, y_test = test_data
    loss, accuracy = model.evaluate(x_test, y_test)
    print("[test_model] evaluate by model.evaluate()")
    print(f"  test loss: {loss:1.4f}")
    print(f"  test accuracy: {accuracy:1.4f}\n")

    predicts = model.predict(x_test)
    print("[test_model] predict by model.predict()")
    print("  prediction shape:", predicts.shape, y_test.shape)
    print("  first 5 predicts:\n", predicts[:5])
    print("  check probability:", np.sum(predicts[:5], axis=1))
    print("  manual accuracy:", np.mean(np.argmax(predicts, axis=1) == y_test))
```

실행 결과는 다음과 같다.

``` python
313/313 [==============================] - 0s 1ms/step - loss: 0.9026 - sparse_categorical_accuracy: 0.6941
[test_model] evaluate by model.evaluate()
  test loss: 0.9026
  test accuracy: 0.6941

[test_model] predict by model.predict()
  prediction shape: (10000, 10) (10000,)
  first 5 predicts:
 [[0.0022 0.0002 0.0032 0.8284 0.0004 0.1584 0.0041 0.0013 0.0013 0.0004]
 [0.0028 0.0137 0.     0.     0.     0.     0.     0.     0.9833 0.0003]
 [0.1002 0.0556 0.0038 0.0031 0.0011 0.0003 0.0011 0.0009 0.773  0.0609]
 [0.7456 0.0009 0.0358 0.     0.0127 0.     0.     0.     0.2047 0.0003]
 [0.     0.0002 0.0113 0.0341 0.1521 0.0018 0.8002 0.0001 0.     0.    ]]
  check probability: [1. 1. 1. 1. 1.]
  manual accuracy: 0.6941
```

---

이후 다음주에...