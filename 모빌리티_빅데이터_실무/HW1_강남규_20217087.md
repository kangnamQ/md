고급딥러닝(2021) HW1
==

### - 20217087	강남규





## 1. keras_classifier.py

### 1.1. 코드

``` python
#-*- encoding: utf-8 -*-

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import pprint
from timeit import default_timer as timer
import matplotlib.pyplot as plt

"""
Common utils
"""


class DurationTime:
    def __init__(self, context):
        self.start = 0
        self.context = context

    # Context Manager의 with이 시작될 때 시작된다.
    def __enter__(self):
        self.start = timer()
        return self # 'as' 있는 부분에 return

    # with 블록이 끝날때 나온다.
    def __exit__(self, type, value, trace_back):
        print(f"{self.context}: {timer() - self.start:1.2f}")


def gpu_config():
    # Gpu를 사용하여 메모리 확장하는 용도로 사용한다.
    # 해줘야 한다.
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # Currently, memory growth needs to be the same across GPUs
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            logical_gpus = tf.config.experimental.list_logical_devices('GPU')
            print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
        except RuntimeError as e:
            # Memory growth must be set before GPUs have been initialized
            print(e)


def load_dataset(dataname="cifar10", show_imgs=True):

    # "cifar10" 데이터셋을 불러온다.
    if dataname == "cifar10":
        dataset = tf.keras.datasets.cifar10
        # 10가지의 클레스로 나누어 져있다.
        class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

    # 만약 "cifar10" 데이터가 아닐경우 ValueError를 뜨게한다.
    else:
        raise ValueError(f"Invalid dataset name: {dataname}")

    (x_train, y_train), (x_test, y_test) = dataset.load_data()
    # 0 ~ 255의 데이터를 가져오게되니 255.0으로 나누어 0 ~ 1사이의 값으로 정규화를 시킨다.
    x_train, x_test = x_train / 255.0, x_test / 255.0

    # y데이터(정답) 값의 차원 [[3],[2],[9]] -> [3, 2, 9] 로 만들어주는 작업
    y_train, y_test = y_train[:, 0], y_test[:, 0]

    print(f"Load {dataname} dataset:", x_train.shape, y_train.shape, x_test.shape, y_test.shape)

    # 위의서의 인자가 show_imgs=True로 들어올 경우 이미지를 보이게 한다.
    if show_imgs:
        show_samples(x_train, y_train, class_names)

    # x_train, y_train, x_test, y_test 의 데이터를 return한다.
    # x와 y의 데이터 갯수는 같아야하고 x_train의 크기와 x_test의 크기도 (32, 32, 3) 같다.
    return (x_train, y_train), (x_test, y_test)


# sample을 보여주는 함수부분.
def show_samples(images, labels, class_names, grid=(3,4)):
    plt.figure(figsize=grid)
    num_samples = grid[0] * grid[1]
    for i in range(num_samples):
        plt.subplot(grid[0], grid[1], i+1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(images[i])
        plt.xlabel(class_names[labels[i]])
    plt.show()

"""
Classifier
"""


def tf2_keras_classifier():
    gpu_config()
    train_data, test_data = load_dataset()
    model = create_model(train_data)
    train_model(model, train_data)
    test_model(model, test_data)


# 받아온 데이터를 사용하여 모델을 만드는 과정
def create_model(dataset, use_add=True):
    x, y = dataset
    # (50000, 32, 32, 3) 으로 나오는 값에서 (32, 32, 3)만 사용하게 슬라이싱.
    # input_shape = x.shape[1:].numpy() # 더 간단하게 사용할 수 있다.
    input_shape = tf.shape(x)[1:].numpy()

    # 0부터 시작해서 최대값 +1을 해준다.
    num_class = tf.reduce_max(y).numpy() + 1
    print(f"[create_model] input shape={input_shape}, num_class={num_class}")

    # 2가지 방법이 존재하며 결과는 동일하다.
    if use_add:
        # Sequential model을 만든뒤 add로 추가하는 방법 
        model = keras.Sequential(name="tf-classifier")
        model.add(layers.Input(shape=input_shape))
        # 필터가 32층, 커널 사이즈가 3 (3*3), 패딩이 same : 바깥쪽에 0을 채워 크기가 같음 (valid : 크기가 작아짐)
        # activation function : relu로 하며 conv1으로 설정.
        model.add(layers.Conv2D(filters=32, kernel_size=3, padding="same", activation="relu", name="conv1"))
        model.add(layers.MaxPool2D(pool_size=(2, 2), name="pooling1"))

        model.add(layers.Conv2D(filters=64, kernel_size=3, padding="same", activation="relu", name="conv2"))
        model.add(layers.MaxPool2D(pool_size=(2, 2), name="pooling2"))

        # (batch, height, width, channel) -> (batch * height * width * channel)
        # 펼치는 작업
        model.add(layers.Flatten(name="flatten"))
        # n : n으로 매칭을 해주어 fully connected layer
        model.add(layers.Dense(units=100, activation="relu", name="dense1"))
        # overfitting 방지
        model.add(keras.layers.Dropout(0.2))
        # 출력의 클래스 갯스만큼 맞춰주는 것.
        # softmax 모든 출력의 합이 1이 되는 비율로 확률이 나온다.
        model.add(layers.Dense(units=num_class, activation="softmax", name="dense2"))
        
    else:
        # layers를 이용하여 추가하는 방법
        # activation에 tf.keras.activations.[사용할 function] 으로 사용가능
        model = keras.Sequential([
            layers.Conv2D(filters=32, kernel_size=3, padding="same", activation="relu", input_shape=input_shape, name="conv1"),
            layers.MaxPool2D(pool_size=(2, 2), name="pooling1"),
            layers.Conv2D(filters=64, kernel_size=3, padding="same", activation="relu", name="conv2"),
            layers.MaxPool2D(pool_size=(2, 2), name="pooling2"),
            layers.Flatten(name="flatten"),
            layers.Dense(units=100, activation="relu", name="dense1"),
            keras.layers.Dropout(0.2),
            layers.Dense(units=num_class, activation="softmax", name="dense2"),
            ],
            name="tf-classifier")

    # 모델의 구조를 터미널에 깔끔하게 프린트 (볼 수 있다)
    model.summary()
    # 모델의 그래프 구조를 그림으로 보여준다.
    keras.utils.plot_model(model, "tf-clsf-model.png")
    return model


def train_model(model, train_data, split_ratio=0.8):
    x, y = train_data

    # overfitting이 일어나는지 확인하기 위해 validation용으로 사용할 데이터 나누기
    # trainlen = x.shape[0] * split_ratio # 로 실행할 시 TypeError 가 발생합니다.
    trainlen = int(tf.shape(x)[0].numpy() * split_ratio)
    x_train, y_train = x[:trainlen], y[:trainlen]
    x_val, y_val = x[trainlen:], y[trainlen:]

    # loss와 optimizer을 지정
    model.compile(
        loss=keras.losses.SparseCategoricalCrossentropy(),
        optimizer=keras.optimizers.RMSprop(),
        # 분류정확도를 측정하는 얘
        metrics=[keras.metrics.SparseCategoricalAccuracy()],
    )

    #  context manager
    with DurationTime("** training time") as duration:
        # 학습
        history = model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))

    # history에는 epoch마다 계산한 평균 loss와 accuracy가 저장되어 있다.
    history = {key: np.array(values) for key, values in history.history.items()}

    # 소수점 4째자리까지 보이게 하기 위해 사용했다.
    np.set_printoptions(precision=4, suppress=True)
    # 좀더 깔끔하게 print하게 해준다.
    pp = pprint.PrettyPrinter(indent=2, width=100, compact=True)
    print("[train_model] training history:")
    pp.pprint(history)


def test_model(model, test_data):
    x_test, y_test = test_data
    loss, accuracy = model.evaluate(x_test, y_test)
    print("[test_model] evaluate by model.evaluate()")
    print(f"  test loss: {loss:1.4f}")
    print(f"  test accuracy: {accuracy:1.4f}\n")

    predicts = model.predict(x_test)
    print("[test_model] predict by model.predict()")
    # 데이터의 shape
    print("  prediction shape:", predicts.shape, y_test.shape)
    # 처음의 5개의 데이터만 보여준다.
    print("  first 5 predicts:\n", predicts[:5])
    # softmax를 사용하였기에 합이 1이 나온다.
    print("  check probability:", np.sum(predicts[:5], axis=1))

    print("  manual accuracy:", np.mean(np.argmax(predicts, axis=1) == y_test))


if __name__ == "__main__":
    tf2_keras_classifier()

```

*  trainlen = x.shape[0] * split_ratio 로 실행할 시 TypeError 가 발생합니다.



### 1.2. 결과

``` python
1 Physical GPUs, 1 Logical GPUs
Load cifar10 dataset: (50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,)
[create_model] input shape=[32 32  3], num_class=10
Model: "tf-classifier"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1 (Conv2D)               (None, 32, 32, 32)        896       
_________________________________________________________________
pooling1 (MaxPooling2D)      (None, 16, 16, 32)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 16, 16, 64)        18496     
_________________________________________________________________
pooling2 (MaxPooling2D)      (None, 8, 8, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 4096)              0         
_________________________________________________________________
dense1 (Dense)               (None, 100)               409700    
_________________________________________________________________
dropout (Dropout)            (None, 100)               0         
_________________________________________________________________
dense2 (Dense)               (None, 10)                1010      
=================================================================
Total params: 430,102
Trainable params: 430,102
Non-trainable params: 0
_________________________________________________________________

Epoch 1/5
1250/1250 [==============================] - 7s 5ms/step - loss: 1.7678 - sparse_categorical_accuracy: 0.3602 - val_loss: 1.1787 - val_sparse_categorical_accuracy: 0.5857
Epoch 2/5
1250/1250 [==============================] - 7s 6ms/step - loss: 1.2030 - sparse_categorical_accuracy: 0.5764 - val_loss: 1.0722 - val_sparse_categorical_accuracy: 0.6266
Epoch 3/5
1250/1250 [==============================] - 5s 4ms/step - loss: 1.0261 - sparse_categorical_accuracy: 0.6413 - val_loss: 0.9773 - val_sparse_categorical_accuracy: 0.6596
Epoch 4/5
1250/1250 [==============================] - 5s 4ms/step - loss: 0.9235 - sparse_categorical_accuracy: 0.6824 - val_loss: 0.9495 - val_sparse_categorical_accuracy: 0.6788
Epoch 5/5
1250/1250 [==============================] - 5s 4ms/step - loss: 0.8376 - sparse_categorical_accuracy: 0.7127 - val_loss: 0.9866 - val_sparse_categorical_accuracy: 0.6739
** training time: 30.08
[train_model] training history:
{ 'loss': array([1.5348, 1.1631, 1.0115, 0.9209, 0.8487]),
  'sparse_categorical_accuracy': array([0.4489, 0.593 , 0.6477, 0.6827, 0.7085]),
  'val_loss': array([1.1787, 1.0722, 0.9773, 0.9495, 0.9866]),
  'val_sparse_categorical_accuracy': array([0.5857, 0.6266, 0.6596, 0.6788, 0.6739])}
313/313 [==============================] - 1s 2ms/step - loss: 0.9928 - sparse_categorical_accuracy: 0.6645
[test_model] evaluate by model.evaluate()
  test loss: 0.9928
  test accuracy: 0.6645

[test_model] predict by model.predict()
  prediction shape: (10000, 10) (10000,)
  first 5 predicts:
 [[0.0107 0.0051 0.0038 0.0566 0.0002 0.0264 0.0005 0.0016 0.8901 0.0049]
 [0.0072 0.2143 0.     0.     0.     0.     0.     0.     0.7776 0.0009]
 [0.1282 0.0311 0.0056 0.0016 0.0002 0.0001 0.0001 0.0001 0.8228 0.0103]
 [0.3267 0.0162 0.016  0.0024 0.0113 0.0001 0.0009 0.0001 0.6203 0.0059]
 [0.0004 0.0005 0.2037 0.2607 0.3605 0.0158 0.1455 0.0032 0.0073 0.0026]]
  check probability: [1. 1. 1. 1. 1.]
  manual accuracy: 0.6645
```



### 1.3. 의미

`batch_size=32, epochs=5` 의 조건일 때
32.20의 트레이닝 시간을 가졌으며 1.5348부터 0.8487까지 loss는 점차 감소하였으며 0.4489부터 0.7085까지 accuracy는 증가하였습니다.
validation결과에서 val_loss값은 감소하다 증가가 일어났으며  val_accuracy은 증가하다 감소하는 모습을 보였습니다.
test 결과 loss 0.9928, accuracy 0.6645의 정확도를 보였습니다.



### 1.4. 개선

개선하는 방법은 대표적으로 5가지 방법을 사용하였습니다.
batch_size 변경, epochs 변경, optimizer 변경, layers 구조 변경, activation function 변경

model을 만들고 add를 하여 추가하는 방법과 model을 만들때 넣어주는 두 방법은 동일하거나 비슷한 결과를 보였습니다.

- traing time : 32.20
- loss : 1.5279, 1.1277, 0.9847, 0.8985, 0.8335
- accuracy : 0.4529, 0.6059, 0.6557, 0.6917, 0.7142
- val_loss : 1.2612, 1.05  , 0.9872, 0.9786, 0.9884
- val_accuracy : 0.5643, 0.6329, 0.6674, 0.6738, 0.6667
- test_loss : 0.9914
- test_accuracy : 0.6646



#### epochs 증가

`batch_size=32, epochs=10`

- traing time : 64.11
- loss : 1.5305, 1.1365, 0.993 , 0.9104, 0.8459, 0.809 , 0.7733, 0.7547, 0.7469, 0.7344
- accuracy : 0.4529, 0.6018, 0.6551, 0.6881, 0.71  , 0.7276, 0.7368, 0.7483, 0.7527, 0.7577
- val_loss : 1.2615, 1.0597, 0.9525, 0.9532, 0.9125, 0.8992, 1.022 , 0.9358, 1.0544, 1.0476
- val_accuracy : 0.5612, 0.626 , 0.6734, 0.6793, 0.6893, 0.707 , 0.6637, 0.7003, 0.6752, 0.6965
- test_loss : 1.0371
- test_accuracy : 0.6979

test결과 loss가 조금 더 증가하였고 accuracy(정확도)는 조금 증가한 것을 확인할 수 있었습니다.
train의 loss는 지속적으로 감소하고 accuracy는 증가하는 추세를 보였지만 
검증값은 6에폭부터 val_loss 는 진동하며 val_accuracy도 진동하며 증가하는 것을 확인할 수 있었습니다.



#### batch_size 증가

`batch_size=64, epochs=5`

- traing time : 17.26
- loss : 1.5765, 1.2001, 1.0387, 0.9343, 0.8438
- accuracy : 0.4316, 0.5757, 0.6357, 0.6755, 0.7061
- val_loss : 1.2832, 1.0979, 0.965 , 0.918 , 0.9569
- val_accuracy : 0.5449, 0.6174, 0.6657, 0.6794, 0.6725
- test_loss : 0.9607
- test_accuracy : 0.6696

batch size를 증가시키니 traing time이 단축되었고 나머지는 비슷한 추세를 띄었습니다.
tess_loss와 test_accuracy는 큰 변화가 없는 것을 확인하였습니다.



##### batch_size 감소

`batch_size=16, epochs=5`를 진행한 결과 traing time은 증가하고 성능은 오히려 안 좋아진 것을 확인하였습니다.

- test_loss : 1.1815
- test_accuracy : 0.6196



##### batch_size 및 epochs 증가

`batch_size=64, epochs=10`를 진행한 결과 성능의 변화는 크게 없는 것을 확인하였습니다.

- test_loss : 0.9877
- test_accuracy : 0.6961





## 2. adv_tf_classifier.py

### 2.1. 코드

``` python
# -*- coding: utf-8 -*-

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
from timeit import default_timer as timer
import matplotlib.pyplot as plt

"""
Common utils
"""


class DurationTime:
	def __init__(self, context):
		self.start = 0
		self.context = context

	# Context Manager의 with이 시작될 때 시작된다.
	def __enter__(self):
		self.start = timer()
		return self  # 'as' 있는 부분에 return

	# with 블록이 끝날때 나온다.
	def __exit__(self, type, value, trace_back):
		print(f"{self.context}: {timer() - self.start:1.2f}")


def gpu_config():
	# Gpu를 사용하여 메모리 확장하는 용도로 사용한다.
	# 해줘야 한다.
	gpus = tf.config.experimental.list_physical_devices('GPU')
	if gpus:
		try:
			# Currently, memory growth needs to be the same across GPUs
			for gpu in gpus:
				tf.config.experimental.set_memory_growth(gpu, True)
			logical_gpus = tf.config.experimental.list_logical_devices('GPU')
			print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
		except RuntimeError as e:
			# Memory growth must be set before GPUs have been initialized
			print(e)


def load_dataset(dataname="cifar10", show_imgs=False):
	# "cifar10" 데이터셋을 불러온다.
	if dataname == "cifar10":
		dataset = tf.keras.datasets.cifar10
		# 10가지의 클레스로 나누어 져있다.
		class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
	# 만약 "cifar10" 데이터가 아닐경우 ValueError를 뜨게한다.
	else:
		raise ValueError(f"Invalid dataset name: {dataname}")

	(x_train, y_train), (x_test, y_test) = dataset.load_data()
	# (50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,)
	# 수업과 동일하게 OOM Error가 떠서 데이터크기를 조절하기로 하였습니다.
	# 테스트 결과 기존데이터의 80%를 사용할 경우(train과 test의) 결과가 정상적으로 나와 80%의 데이터를 가지고 진행하였습니다.
	x_train, y_train, x_test, y_test = x_train[:40000, :, :, :], y_train[:40000, :], \
									   x_test[:8000, :, :, :], y_test[:8000, :]
	# 0 ~ 255의 데이터를 가져오게되니 255.0으로 나누어 0 ~ 1사이의 값으로 정규화를 시킨다.
	x_train, x_test = x_train / 255.0, x_test / 255.0
	# y데이터(정답) 값의 차원 [[3],[2],[9]] -> [3, 2, 9] 로 만들어주는 작업
	y_train, y_test = y_train[:, 0], y_test[:, 0]
	print(f"Load {dataname} dataset:", x_train.shape, y_train.shape, x_test.shape, y_test.shape)
	# 위의서의 인자가 show_imgs=True로 들어올 경우 이미지를 보이게 한다.
	if show_imgs:
		show_samples(x_train, y_train, class_names)
	# x_train, y_train, x_test, y_test 의 데이터를 return한다.
	# x와 y의 데이터 갯수는 같아야하고 x_train의 크기와 x_test의 크기도 (32, 32, 3) 같다.
	return (x_train, y_train), (x_test, y_test)


# sample을 보여주는 함수부분.
def show_samples(images, labels, class_names, grid=(3, 4)):
	plt.figure(figsize=grid)
	num_samples = grid[0] * grid[1]
	for i in range(num_samples):
		plt.subplot(grid[0], grid[1], i + 1)
		plt.xticks([])
		plt.yticks([])
		plt.grid(False)
		plt.imshow(images[i])
		plt.xlabel(class_names[labels[i]])
	plt.show()


"""
Classifier
"""


class AdvancedClassifier:
	# 설정값을 초기변수로 지정
	# val_ratio가
	def __init__(self, batch_size=32, val_ratio=0.2):
		# 밑에 build_model 에서 만드는 model만 쓰고 이부분은 초기변수지정이다. (None으로 해도 된다.)
		self.model = keras.Model()
		self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
		self.optimizer = tf.keras.optimizers.RMSprop()
		self.batch_size = batch_size
		self.val_ratio = val_ratio

	def build_model(self, x, y):
		input_shape = x.shape[1:]
		num_class = tf.reduce_max(y).numpy() + 1
		input_tensor = layers.Input(shape=input_shape)
		x = layers.Conv2D(filters=32, kernel_size=3, padding="same", activation="relu", name="conv1")(input_tensor)
		x = layers.MaxPool2D(pool_size=(2, 2), name="pooling1")(x)

		x = layers.Conv2D(filters=64, kernel_size=3, padding="same", activation="relu", name="conv2")(x)
		x = layers.MaxPool2D(pool_size=(2, 2), name="pooling2")(x)

		x = layers.Flatten(name="flatten")(x)
		x = layers.Dense(units=100, activation="relu", name="dense1")(x)
		x = layers.Dropout(0.2)(x)

		output_tensor = layers.Dense(units=num_class, activation="softmax", name="dense2")(x)
		self.model = keras.Model(inputs=input_tensor, outputs=output_tensor, name="tf-classifier")
		self.model.summary()
		keras.utils.plot_model(self.model, "tf-clsf-model-adv.png")

	def train(self, x, y, epochs):
		# trainlen = int(tf.shape(x)[0].numpy() * (1 - self.val_ratio))을 사용하면 결과가 안나옴.
		trainlen = int(x.shape[0] * (1 - self.val_ratio))
		# print("trainlen", trainlen, type(trainlen))
		x_train, y_train = x[:trainlen], y[:trainlen]
		print(x_train.shape, y_train.shape)
		x_val, y_val = x[trainlen:], y[trainlen:]
		print(x_val.shape, y_val.shape)
		# 데이터 셋을 가져온다. 생성자 대신 @staticmethod를 사용하여 class를 구현해 놓은 함수다.
		dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
		# 데이터를 섞어준다. 훈련할때만 쓰고 테스트 할 때는 사용하지 않는 부분이다.
		dataset = dataset.shuffle(200).batch(self.batch_size)
		with DurationTime("** training time") as duration:
			for epoch in range(epochs):
				# 배치단위로 들어가서 dataset이 끝날때 까지 돌아간다.
				for x_batch, y_batch in dataset:
					self.train_batch_graph(x_batch, y_batch)
				# 검증(val)을 통해 성능확인을 하는 부분이다.
				loss, accuracy = self.evaluate(x_val, y_val, verbose=False)
				print(f"[Training] epoch={epoch}, val_loss={loss}, val_accuracy={accuracy}")

	# 데코레이터가 붙으면 graph모드가 되며 복잡한 모드에서는 더 빠르지만 간단한 모델은 eager 모드가 더 빠를 수 있다.
	# train 위에 데코레이터를 붙이면 속도는 더 빨라질 수 있으나 중간에 로그(print)를 보기 위하여 이렇게 사용한다.
	# 또한 데코레이터가 붙어있는 값은 print를 해도 값이 안나오거나 정보가 없다.
	@tf.function
	def train_batch_graph(self, x_batch, y_batch):
		# context 안에서 발생하는 모든 연산의 미분값을 tape 객체에 기록한다.
		with tf.GradientTape() as tape:
			# training=True is only needed if there are layers with different
			# behavior during training versus inference (e.g. Dropout).
			predictions = self.model(x_batch, training=True)
			loss = self.loss_object(y_batch, predictions)
		# 여러 튜플들이 나오는 trainable_variables(weight)를 각각 loss로 나눈다. (dL/dw1, ...)
		gradients = tape.gradient(loss, self.model.trainable_variables)
		# 얻은 gradient들과 weight를 짝을 맞춰 사용하기 위해 zip으로 묶어서 만든다. (주로 for문에서 사용한다.)
		self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

	def evaluate(self, x, y_true, verbose=True):
		if verbose:
			print("[evaluate] predict by model.__call__()")
		# self.model(x)에 모델을 함수처럼 사용하여 __call__함수를 사용한 것임.
		y_pred = self.model(x)
		accuracy = np.mean(np.argmax(y_pred, axis=1) == y_true)
		loss = self.loss_object(y_true, y_pred)
		if verbose:
			np.set_printoptions(precision=4, suppress=True)
			print("  prediction shape:", y_pred.shape, y_true.shape)
			print("  first 5 predicts:\n", y_pred[:5].numpy())
			print("  check probability:", np.sum(y_pred[:5], axis=1))
			print(f"  loss={loss:1.4f}, accuracy={accuracy:1.4f}")
		return loss, accuracy


def tf2_advanced_classifier():
	gpu_config()
	(x_train, y_train), (x_test, y_test) = load_dataset("cifar10")
	clsf = AdvancedClassifier()
	clsf.build_model(x_train, y_train)
	clsf.train(x_train, y_train, 5)
	clsf.evaluate(x_test, y_test)


if __name__ == "__main__":
	tf2_advanced_classifier()

```



### 2.2. 결과

``` python
1 Physical GPUs, 1 Logical GPUs
Load cifar10 dataset: (40000, 32, 32, 3) (40000,) (8000, 32, 32, 3) (8000,)
Model: "tf-classifier"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 32, 32, 32)        896       
_________________________________________________________________
pooling1 (MaxPooling2D)      (None, 16, 16, 32)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 16, 16, 64)        18496     
_________________________________________________________________
pooling2 (MaxPooling2D)      (None, 8, 8, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 4096)              0         
_________________________________________________________________
dense1 (Dense)               (None, 100)               409700    
_________________________________________________________________
dropout (Dropout)            (None, 100)               0         
_________________________________________________________________
dense2 (Dense)               (None, 10)                1010      
=================================================================
Total params: 430,102
Trainable params: 430,102
Non-trainable params: 0
_________________________________________________________________
(32000, 32, 32, 3) (32000,)
(8000, 32, 32, 3) (8000,)

[Training] epoch=0, val_loss=1.2817704677581787, val_accuracy=0.5385
[Training] epoch=1, val_loss=1.1395236253738403, val_accuracy=0.60375
[Training] epoch=2, val_loss=1.0366662740707397, val_accuracy=0.644
[Training] epoch=3, val_loss=1.00533926486969, val_accuracy=0.652875
[Training] epoch=4, val_loss=0.9814902544021606, val_accuracy=0.669875
** training time: 21.56
[evaluate] predict by model.__call__()
  prediction shape: (8000, 10) (8000,)
  first 5 predicts:
 [[0.001  0.0003 0.0027 0.7659 0.0001 0.1444 0.0056 0.0012 0.0782 0.0007]
 [0.0116 0.6997 0.     0.     0.     0.     0.     0.     0.2886 0.0002]
 [0.2023 0.1818 0.0042 0.0009 0.002  0.0001 0.0002 0.0008 0.6017 0.0062]
 [0.6496 0.0096 0.0093 0.0001 0.0316 0.     0.0001 0.0001 0.2994 0.0002]
 [0.     0.     0.038  0.0382 0.1216 0.0051 0.7969 0.0001 0.     0.    ]]
  check probability: [1. 1. 1. 1. 1.]
  loss=0.9457, accuracy=0.6875
```



### 2.3. 의미

`model.fit()`을 진행하며 학습이 잘못 되었을 때 어디서 무엇이 잘못됐는지 알기 어렵기 때문에 중간결과물을 저장하고 싶거나 체크포인트를 저장하고 싶을 때 사용하는 방법.
Keras에도 callback객체를 이용하여 사용할 수 있는 모델들이 있지만 이를 배우는데 더 어렵고 직접 구현하는 것이 좋아 해당부분을 구현하는 방법을 중점으로 진행되었다.
검출 모델과 같은 복잡한 모델은 Keras API를 모델을 정의하는 레이어 객체를 만드는 정도에만 사용하고 loss나 metric등의 계산은 텐서플로의 함수들을 사용하는 것이 좋다.
1번과 다르게 Class로 만들어져 진행이 되었다.

진행하면서 OOM관련 Error가 떠서 여러 방법을 시도해 보다가 `load_dataset`함수 부분에서 데이터의 크기를 조절하였습니다. 크기를 조절함에 있어 train과 test의 비율로 감소시켰습니다.
train : 40000, test : 8000개의 갯수가 안정적으로 결과를 냈기 때문에 해당 방법으로 진행하였습니다.

- train : 50000 , test : 10000	X
- train : 45000 , test : 9000	X
- train : 40000 , test : 8000	O
- train : 35000 , test : 7000	O



### 2.4. 개선

#### Dataset의 갯수 변경

데이터의 갯수를 바꾸어 실행해본 결과이며  이는 데이터의 갯수가 결과에 영향을 미치는 것을 확인할 수 있었습니다.

- train : 40000 , test : 8000	=	loss=0.9457, accuracy=0.6875
- train : 35000 , test : 7000	=	loss=1.0231, accuracy=0.6530



#### batch_size 증가

`batch_size=64` => OOM Error



#### epochs 증가

`epochs=10` 	=	loss=1.0685, accuracy=0.6931

loss나 accuracy모두 감소하다가 증가하거나, 증가하다가 감소하거나 진동하는 추세를 보이며 진행됬으며
epochs이 5일때와 비교하면, loss는 증가하고, accuracy도 증가하여 큰 차이가 없는 것을 확인하였습니다.





## 3. torch_classifier.py

### 3.1. 코드

#### 3.1.1) Mix Model

``` python
# -*- coding: utf-8 -*-

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from timeit import default_timer as timer

"""
Common utils
"""


class DurationTime:
    def __init__(self, context):
        self.start = 0
        self.context = context

    def __enter__(self):        # entering 'with' context
        self.start = timer()
        return self             # pass object by 'as'

    def __exit__(self, type, value, trace_back):    # exiting 'with' context
        print(f"{self.context}: {timer() - self.start:1.2f}")

def load_dataset(dataname="cifar10", show_imgs=False):
    if dataname == "cifar10":
        # 실제로 사용할 때는 아래와 같이 바로 dataset 객체를 받아 사용할 수 있다.
        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)
        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)
        class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
    else:
        raise ValueError(f"Invalid dataset name: {dataname}")

    # pytorch uses image shape like (batch, channel, height, width)
    # 원본 dataset을 가지고와서 numpydata로 return하도록 만드는 작업을 진행한다.
    # dataset을 다시 만드는 과정이다.
    # torchvision으로 데이터를 가져올 때 channer last 형식으로 가져오게 되어 channer first 형식으로 바꿔주는 과정을 가진다.
    x_train = np.transpose(np.array(trainset.data), (0, 3, 1, 2))
    y_train = np.array(trainset.targets)
    x_test = np.transpose(np.array(testset.data), (0, 3, 1, 2))
    y_test = np.array(testset.targets)
    # uint8의 데이터(0~255)를 곧바로 넣으면 값이 너무 커서 안되기 때문에 -1~1사이의 값으로 데이터를 만들어주는 과정이다.
    x_train, x_test = x_train / 255.0 * 2. - 1., x_test / 255.0 * 2. - 1.
    print(f"Load {dataname} dataset:", x_train.shape, y_train.shape, x_test.shape, y_test.shape)
    if show_imgs:
        show_samples(trainset.data, trainset.targets, class_names)
    return (x_train, y_train), (x_test, y_test)


def show_samples(images, labels, class_names, grid=(3,4)):
    plt.figure(figsize=grid)
    num_samples = grid[0] * grid[1]
    for i in range(num_samples):
        plt.subplot(grid[0], grid[1], i+1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(images[i])
        plt.xlabel(class_names[labels[i]])
    plt.show()


"""
Classifier
"""

# Normal
# class TorchClsfModel(nn.Module):
#     def __init__(self):
#         super(TorchClsfModel, self).__init__()
#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3,
#                                padding=1, padding_mode='zeros')
#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
#         self.conv2 = nn.Conv2d(32, 64, 3, padding=1, padding_mode='zeros')
#         self.pool2 = nn.MaxPool2d(2, 2)
#         self.fc1 = nn.Linear(64 * 8 * 8, 100)
#         self.fc2 = nn.Linear(100, 10)
#         self.softmax = nn.Softmax()
#
#     def forward(self, x):
#         # relu는 학습되는 파라미터가 없어서 함수로만 실행된다.
#         # 학습되거나 유지되어야 하는 값은 self로 지정하여 값을 유지하여 진행한다.
#         x = self.pool1(F.relu(self.conv1(x)))
#         x = self.pool2(F.relu(self.conv2(x)))
#         # reshape와 비슷한 기능, 메모리는 그대로 납두고 메모리를 읽어들이는 구조만 바꾼다.
#         x = x.view(-1, 64 * 8 * 8)
#         x = F.relu(self.fc1(x))
#         x = F.dropout(x)
#         x = F.softmax(self.fc2(x), dim=1)
#         return x

# Mixed Model
# nn.Module을 받아 모델을 만드는 class
class TorchClsfModel(nn.Module):
    def __init__(self):
        super(TorchClsfModel, self).__init__()
        self.conv_relu_pool1 = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3,
                      padding=1, padding_mode='zeros'),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.ReLU(),
        )
        self.conv_relu_pool2 = nn.Sequential(
            nn.Conv2d(32, 64, 3, padding=1, padding_mode='zeros'),
            nn.MaxPool2d(2, 2),
            nn.ReLU(),
        )
        self.fc1 = nn.Linear(64 * 8 * 8, 100)
        self.fc2 = nn.Linear(100, 10)
        self.softmax = nn.Softmax()

    def forward(self, x):
        x = self.conv_relu_pool1(x)
        x = self.conv_relu_pool2(x)
        x = x.view(-1, 64 * 8 * 8)
        x = F.relu(self.fc1(x))
        x = F.dropout(x)
        x = F.softmax(self.fc2(x), dim=1)
        return x

# class TorchClsfModel 에서 만든 모델을 사용하는 class
class TorchClassifier:
    def __init__(self, model, batch_size=32, val_ratio=0.2):
        self.model = model
        self.loss_object = nn.CrossEntropyLoss()
        self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.001)
        self.batch_size = batch_size
        self.val_ratio = val_ratio

    def train(self, x, y, epochs):
        x, y = torch.from_numpy(x).float(), torch.from_numpy(y)
        trainlen = int(x.shape[0] * (1 - self.val_ratio))
        x_train, y_train = x[:trainlen], y[:trainlen]
        trainset = torch.utils.data.TensorDataset(x_train, y_train)
        trainloader = torch.utils.data.DataLoader(trainset, batch_size=self.batch_size, shuffle=True, num_workers=2)
        x_val, y_val = x[trainlen:], y[trainlen:]

        with DurationTime("** training time"):
            for epoch in range(epochs):
                for x_batch, y_batch in trainloader:
                    self.train_batch(x_batch, y_batch)

                loss, accuracy = self.evaluate(x_val, y_val, verbose=False)
                print(f"[Training] epoch={epoch}, val_loss={loss:1.4f}, val_accuracy={accuracy:1.4f}")

    def train_batch(self, x_batch, y_batch):
        # zero the parameter gradients
        self.optimizer.zero_grad()
        # forward + backward + optimize
        y_pred = self.model(x_batch)
        loss = self.loss_object(y_pred, y_batch)
        loss.backward()
        self.optimizer.step()

    def evaluate(self, x, y_true, verbose=True):
        if isinstance(x, np.ndarray):
            x, y_true = torch.from_numpy(x).float(), torch.from_numpy(y_true)

        y_pred = self.model(x)
        accuracy = torch.mean((torch.argmax(y_pred, dim=1) == y_true).float())
        loss = self.loss_object(y_pred, y_true)
        if verbose:
            np.set_printoptions(precision=4, suppress=True)
            print("  prediction shape:", y_pred.shape, y_true.shape)
            print("  first 5 predicts:\n", y_pred[:5].detach().numpy())
            print("  check probability:", torch.sum(y_pred[:5], dim=1))
            print(f"  loss={loss.detach().numpy():1.4f}, accuracy={accuracy.detach().numpy():1.4f}")
        return loss, accuracy


def torch_classifier():
    (x_train, y_train), (x_test, y_test) = load_dataset("cifar10")
    model = TorchClsfModel()
    clsf = TorchClassifier(model)
    clsf.train(x_train, y_train, 5)
    clsf.evaluate(x_test, y_test)


if __name__ == "__main__":
    torch_classifier()
```

#### 3.1.2) Normal model

``` python
# Normal
class TorchClsfModel(nn.Module):
    def __init__(self):
        super(TorchClsfModel, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3,
                               padding=1, padding_mode='zeros')
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1, padding_mode='zeros')
        self.pool2 = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 8 * 8, 100)
        self.fc2 = nn.Linear(100, 10)
        self.softmax = nn.Softmax()

    def forward(self, x):
        # relu는 학습되는 파라미터가 없어서 함수로만 실행된다.
        # 학습되거나 유지되어야 하는 값은 self로 지정하여 값을 유지하여 진행한다.
        x = self.pool1(F.relu(self.conv1(x)))
        x = self.pool2(F.relu(self.conv2(x)))
        # reshape와 비슷한 기능, 메모리는 그대로 납두고 메모리를 읽어들이는 구조만 바꾼다.
        x = x.view(-1, 64 * 8 * 8)
        x = F.relu(self.fc1(x))
        x = F.dropout(x)
        x = F.softmax(self.fc2(x), dim=1)
        return x
```

Mix model 대신 사용하면 된다.



### 3.2. 결과

#### 3.2.1) Mix Model

``` python
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz
100.0%Extracting ./data/cifar-10-python.tar.gz to ./data
Files already downloaded and verified
Load cifar10 dataset: (50000, 3, 32, 32) (50000,) (10000, 3, 32, 32) (10000,)
[Training] epoch=0, val_loss=2.0530, val_accuracy=0.4027
[Training] epoch=1, val_loss=1.9974, val_accuracy=0.4580
[Training] epoch=2, val_loss=1.9587, val_accuracy=0.5025
[Training] epoch=3, val_loss=1.9282, val_accuracy=0.5336
[Training] epoch=4, val_loss=1.9177, val_accuracy=0.5418
** training time: 78.52
  prediction shape: torch.Size([10000, 10]) torch.Size([10000])
  first 5 predicts:
 [[0.     0.     0.     0.9999 0.     0.0001 0.     0.     0.     0.    ]
 [0.0495 0.9504 0.     0.     0.     0.     0.     0.     0.0001 0.    ]
 [0.001  0.     0.     0.     0.     0.     0.     0.     0.999  0.    ]
 [0.9959 0.     0.     0.     0.     0.     0.     0.     0.0041 0.    ]
 [0.     0.     0.     0.     0.0009 0.     0.9991 0.     0.     0.    ]]
  check probability: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)
  loss=1.9241, accuracy=0.5326

```

#### 3.2.2) Normal Model

```python
Files already downloaded and verified
Files already downloaded and verified
Load cifar10 dataset: (50000, 3, 32, 32) (50000,) (10000, 3, 32, 32) (10000,)
[Training] epoch=0, val_loss=2.0228, val_accuracy=0.4347
[Training] epoch=1, val_loss=1.9682, val_accuracy=0.4924
[Training] epoch=2, val_loss=1.9447, val_accuracy=0.5141
[Training] epoch=3, val_loss=1.9350, val_accuracy=0.5246
[Training] epoch=4, val_loss=1.9249, val_accuracy=0.5319
** training time: 88.42
  prediction shape: torch.Size([10000, 10]) torch.Size([10000])
  first 5 predicts:
 [[0.     0.     0.     1.     0.     0.     0.     0.     0.     0.    ]
 [0.     0.9276 0.     0.     0.     0.     0.     0.     0.0724 0.    ]
 [0.0913 0.0015 0.     0.     0.     0.     0.     0.     0.9072 0.    ]
 [0.2579 0.     0.     0.     0.     0.     0.     0.     0.7421 0.    ]
 [0.     0.     0.     0.     0.     0.     1.     0.     0.     0.    ]]
  check probability: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)
  loss=1.9179, accuracy=0.5395
```



### 3.3. 의미

1. Pytorch를 사용하였으며 2번에서 다룬 `tensorflow`와는 조금 다르게 진행되는 부분이 있었습니다..
   `tensorflow`를 사용한   `AdvancedClassifier`에서는 한 class안에서 모델을 만들고 사용하였지만
   `pytorch`를 사용할 때는 `class TorchClsfModel(nn.Module):`에서 모델객체를 직접 클래스로 만들고
   `class TorchClassifier:`안에서 만들어놓은 모델을 사용하는 구조로 진행됩니다.

2. 데이터를 한 단위씩 꺼내거나 읽는 기능에 대해선
   `tensorflow`에서 `tf.data.Dataset`class의 데이터를 `.from_tensor_slices()`를 사용해서 dataset이라는 객체를 만들어 사용한것과 같이 
   `pytorch`에서는 `torch.utils.data.Dataset` class를 상속받은 class에서 구현이 되어야 합니다.

   또한 `tensorflow`에서 한 shuffling, vbatching 등의 기능은 `torch.utils.data.Dataset`객체에서 하는 것이 아닌 해당 객체보다 상위의 `torch.utils.data.DataLoader`라는 class(객체)에서 진행됩니다.
   정리하자면 pytorch에서는 Dataset을 상속받는 클래스를 구현하여 데이터를 읽고, DataLoader 클래스를 이용하여 학습데이터로 사용하여 진행합니다.

3. `CIFAR10`data는 (channel, height, width)의 channer first 형식으로 저장되어 있지만 
   `torchvision`으로 데이터를 가지고 올때에 (height, width, channel)의 channer last형식으로 가지고 오게되어
   `pytorch`에서 사용할 수 있게하기 위하여 `np.transpose()`함수를 사용하여 변환해주는 과정을 가집니다.
   (channer이 first형식인지, last형식인지 파악하며 진행하는 것이 중요합니다.)

4. `x_train / 255.0 * 2. - 1.`과정은 uint8의 데이터(0~255)를 곧바로 넣으면 값이 너무 커서 안되기 때문에 -1 ~ 1사이의 값으로 데이터를 만들어주는 과정이다.

5. `TorchClsfModel`class에서 모델을 정의하는 과정은  `tensorflow`보다 간단한 부분이 있습니다.

6. padding 과정 중 외각쪽의 값들을 점점 잃을 수 있거나 값이 1개만 나올 수 있어 `padding_mode='zeros'`를 설정을 하였고 `padding=1`을 주어 외각에 한겹으로 zero padding을 진행합니다.

7. `import torch.nn.functional as F`를 사용하여 `F.relu(self.conv1(x))`이런식으로 사용한 `relu`는 학습되는 파라미터가 없어서 함수로만 실행하게됩니다. 학습되거나 유지되거나 계속 사용해야 하는 값은 self로 지정하여 값을 유지하여 진행됩니다.

8. `.view()`기능은 reshape와 비슷한 기능으로 메모리는 그대로 납두고 메모리를 읽어들이는 구조만 바꾸는 기능입니다. 차원의 순서를 변경할때는 `tranpose`같은 함수를 사용하지만 단순히 차원을 합치거나 쪼개는 부분에 있어서는 `view()`의 속도가 메모리를 추가적으로 할당하지 않아 더욱 빠른 속도를 가질 수 있습니다.
9. `pytorch`에서는 입력 채널, 입력 벡터의 크기와 출력 채널(노드수), 출력 벡터의 크기를 적어줘야 하는 부분이 있다.
   (다르게 적는다면 데이터가 달라질 수 있다.)
10. 해당 모델에서의 결과로는 `tensorflow`에 비해 시간도 오래 걸리고 정확도도 낮으며 성능이 안좋게 나오는 것을 확인할 수 있습니다. 다만 작은 모델이라 두 프레임워크를 비교하는 것은 어렵다고합니다.



### 3.4. 개선

#### Model을 만드는 방법 변경

Model을 두가지로 나누어서 Normal과 Mix로 진행을 해본 결과 값은 동일하거나 비슷한 결과를 보였습니다.
다만 Mix모델 쪽의 training time이 조금 더 빠른것을 확인할 수 있습니다.

두 경우 모두 loss값은 줄고 accuracy값은 증가하는 추세를 보였으며 
test결과 loss : 1.9241, accuracy : 0.5326의 값을 보였습니다.



#### epochs 증가

`epochs=10` 	=	loss=1.8733, accuracy=0.5847

loss나 accuracy모두 감소하다가 증가하거나, 증가하다가 감소하거나 진동하는 추세를 보이며 진행됬으며
epochs이 5일때와 비교하면, loss는 감소하고, accuracy도 증가하여 조금의 성능 상승은 있지만 큰 차이가 없는 것을 확인하였습니다.





## 4. training_loop.py

### 4.1. 코드

#### 4.1.1) Low-level handling of metrics

``` python
# -*- coding: utf-8 -*-

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import time

inputs = keras.Input(shape=(784,), name="digits")
x = layers.Dense(64, activation="relu", name="dense_1")(inputs)
x = layers.Dense(64, activation="relu", name="dense_2")(x)
outputs = layers.Dense(10, name="predictions")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

optimizer = keras.optimizers.SGD(learning_rate=1e-3)

loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

batch_size = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_dataset = val_dataset.batch(batch_size)

epochs = 2
for epoch in range(epochs):
    print(f"\nStart of epoch {epoch}")
    start_time = time.time()

    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        with tf.GradientTape() as tape:
            logits = model(x_batch_train, training=True)
            loss_value = loss_fn(y_batch_train, logits)
        grads = tape.gradient(loss_value, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))

        train_acc_metric.update_state(y_batch_train, logits)

        if step % 200 == 0:
            print(f"Training loss (for one batch) at step {step} : %.4f" % (float(loss_value)))
            print(f"Seen so far: {((step + 1) * batch_size)} samples")

    train_acc = train_acc_metric.result()
    print(f"Training acc over epoch: %.4f" % float(train_acc))

    train_acc_metric.reset_states()

    for x_batch_val, y_batch_val in val_dataset:
        val_logits = model(x_batch_val, training=False)
        val_acc_metric.update_state(y_batch_val, val_logits)
    val_acc = val_acc_metric.result()
    val_acc_metric.reset_states()
    print(f"Validation acc: %.4f" % float(val_acc))
    print(f"Time taken: %.2fs" % (time.time() - start_time))
```

#### 4.1.2) Use tf.function

``` python
# -*- coding: utf-8 -*-

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import time


inputs = keras.Input(shape=(784,), name="digits")
x = layers.Dense(64, activation="relu", name="dense_1")(inputs)
x = layers.Dense(64, activation="relu", name="dense_2")(x)
outputs = layers.Dense(10, name="predictions")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

optimizer = keras.optimizers.SGD(learning_rate=1e-3)

loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

batch_size = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_dataset = val_dataset.batch(batch_size)

@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)
    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    train_acc_metric.update_state(y, logits)
    return loss_value

@tf.function
def test_step(x, y):
    val_logits = model(x, training=False)
    val_acc_metric.update_state(y, val_logits)


epochs = 2
for epoch in range(epochs):
    print(f"\nStart of epoch {epoch}")
    start_time = time.time()

    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        loss_value = train_step(x_batch_train, y_batch_train)

        if step % 200 == 0:
            print(f"Training loss (for one batch) at step {step} : %.4f" % (float(loss_value)))
            print(f"Seen so far: {((step + 1) * batch_size)} samples")

    train_acc = train_acc_metric.result()
    print(f"Training acc over epoch: %.4f" % float(train_acc))

    train_acc_metric.reset_states()

    for x_batch_val, y_batch_val in val_dataset:
        test_step(x_batch_val, y_batch_val)

    val_acc = val_acc_metric.result()
    val_acc_metric.reset_states()
    print(f"Validation acc: %.4f" % float(val_acc))
    print(f"Time taken: %.2fs" % (time.time() - start_time))

```

#### 4.1.3) Handling of losses tracked by the model

```python
# -*- coding: utf-8 -*-

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import time


class ActivityRegularizationLayer(layers.Layer):
    def call(self, inputs):
        self.add_loss(1e-2 * tf.reduce_sum(inputs))
        return inputs

inputs = keras.Input(shape=(784,), name="digits")
x = layers.Dense(64, activation="relu", name="dense_1")(inputs)
x = ActivityRegularizationLayer()(x)
x = layers.Dense(64, activation="relu", name="dense_2")(x)
outputs = layers.Dense(10, name="predictions")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

optimizer = keras.optimizers.SGD(learning_rate=1e-3)

loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

batch_size = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_dataset = val_dataset.batch(batch_size)

@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss_value = loss_fn(y, logits)
        # 전달 패스를 진행하는 동안 발생한 추가 손실을 추가
        loss_value += sum(model.losses)
    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    train_acc_metric.update_state(y, logits)
    return loss_value

@tf.function
def test_step(x, y):
    val_logits = model(x, training=False)
    val_acc_metric.update_state(y, val_logits)


epochs = 2
for epoch in range(epochs):
    print(f"\nStart of epoch {epoch}")
    start_time = time.time()

    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        loss_value = train_step(x_batch_train, y_batch_train)

        if step % 200 == 0:
            print(f"Training loss (for one batch) at step {step} : %.4f" % (float(loss_value)))
            print(f"Seen so far: {((step + 1) * batch_size)} samples")

    train_acc = train_acc_metric.result()
    print(f"Training acc over epoch: %.4f" % float(train_acc))

    train_acc_metric.reset_states()

    for x_batch_val, y_batch_val in val_dataset:
        test_step(x_batch_val, y_batch_val)

    val_acc = val_acc_metric.result()
    val_acc_metric.reset_states()
    print(f"Validation acc: %.4f" % float(val_acc))
    print(f"Time taken: %.2fs" % (time.time() - start_time))
```

#### 4.1.4) A GAN training loop from scratch

```python
# -*- coding: utf-8 -*-

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import os


discriminator = keras.Sequential(
    [
        keras.Input(shape=(28, 28, 1)),
        layers.Conv2D(64, (3, 3), strides=(2, 2), padding="same"),
        layers.LeakyReLU(alpha=0.2),
        layers.Conv2D(128, (3, 3), strides=(2, 2), padding="same"),
        layers.LeakyReLU(alpha=0.2),
        layers.GlobalMaxPooling2D(),
        layers.Dense(1),
    ],
    name="discriminator",
)
# discriminator.summary()

latent_dim = 128

generator = keras.Sequential(
    [
        keras.Input(shape=(latent_dim,)),
        # We want to generate 128 coefficients to reshape into a 7x7x128 map
        layers.Dense(7 * 7 * 128),
        layers.LeakyReLU(alpha=0.2),
        layers.Reshape((7, 7, 128)),
        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding="same"),
        layers.LeakyReLU(alpha=0.2),
        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding="same"),
        layers.LeakyReLU(alpha=0.2),
        layers.Conv2D(1, (7, 7), padding="same", activation="sigmoid"),
    ],
    name="generator",
)


# Instantiate one optimizer for the discriminator and another for the generator.
d_optimizer = keras.optimizers.Adam(learning_rate=0.0003)
g_optimizer = keras.optimizers.Adam(learning_rate=0.0004)

# Instantiate a loss function.
loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)


@tf.function
def train_step(real_images):
    # Sample random points in the latent space
    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))
    # Decode them to fake images
    generated_images = generator(random_latent_vectors)
    # Combine them with real images
    combined_images = tf.concat([generated_images, real_images], axis=0)

    # Assemble labels discriminating real from fake images
    labels = tf.concat(
        [tf.ones((batch_size, 1)), tf.zeros((real_images.shape[0], 1))], axis=0
    )
    # Add random noise to the labels - important trick!
    labels += 0.05 * tf.random.uniform(labels.shape)

    # Train the discriminator
    with tf.GradientTape() as tape:
        predictions = discriminator(combined_images)
        d_loss = loss_fn(labels, predictions)
    grads = tape.gradient(d_loss, discriminator.trainable_weights)
    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))

    # Sample random points in the latent space
    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))
    # Assemble labels that say "all real images"
    misleading_labels = tf.zeros((batch_size, 1))

    # Train the generator (note that we should *not* update the weights
    # of the discriminator)!
    with tf.GradientTape() as tape:
        predictions = discriminator(generator(random_latent_vectors))
        g_loss = loss_fn(misleading_labels, predictions)
    grads = tape.gradient(g_loss, generator.trainable_weights)
    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))
    return d_loss, g_loss, generated_images


# Prepare the dataset. We use both the training & test MNIST digits.
batch_size = 64
(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()
all_digits = np.concatenate([x_train, x_test])
all_digits = all_digits.astype("float32") / 255.0
all_digits = np.reshape(all_digits, (-1, 28, 28, 1))
dataset = tf.data.Dataset.from_tensor_slices(all_digits)
dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)

epochs = 1  # In practice you need at least 20 epochs to generate nice digits.
save_dir = "./"

for epoch in range(epochs):
    print("\nStart epoch", epoch)

    for step, real_images in enumerate(dataset):
        # Train the discriminator & generator on one batch of real images.
        d_loss, g_loss, generated_images = train_step(real_images)

        # Logging.
        if step % 200 == 0:
            # Print metrics
            print("discriminator loss at step %d: %.2f" % (step, d_loss))
            print("adversarial loss at step %d: %.2f" % (step, g_loss))

            # Save one generated image
            img = tf.keras.preprocessing.image.array_to_img(
                generated_images[0] * 255.0, scale=False
            )
            img.save(os.path.join(save_dir, "generated_img" + str(step) + ".png"))

        # To limit execution time we stop after 10 steps.
        # Remove the lines below to actually train the model!
        if step > 10:
            break
```



### 4.2. 결과

#### 4.2.1) Low-level handling of metrics

``` python
Start of epoch 0
Training loss (for one batch) at step 0 : 83.1320
Seen so far: 64 samples
Training loss (for one batch) at step 200 : 1.5567
Seen so far: 12864 samples
Training loss (for one batch) at step 400 : 1.4023
Seen so far: 25664 samples
Training loss (for one batch) at step 600 : 1.1630
Seen so far: 38464 samples
Training acc over epoch: 0.6603
Validation acc: 0.7799
Time taken: 3.73s

Start of epoch 1
Training loss (for one batch) at step 0 : 0.8439
Seen so far: 64 samples
Training loss (for one batch) at step 200 : 0.5479
Seen so far: 12864 samples
Training loss (for one batch) at step 400 : 0.7670
Seen so far: 25664 samples
Training loss (for one batch) at step 600 : 0.7305
Seen so far: 38464 samples
Training acc over epoch: 0.8071
Validation acc: 0.8452
Time taken: 3.49s
```

#### 4.2.2) Use tf.function

``` python
Start of epoch 0
Training loss (for one batch) at step 0 : 111.2971
Seen so far: 64 samples
Training loss (for one batch) at step 200 : 1.3676
Seen so far: 12864 samples
Training loss (for one batch) at step 400 : 1.0721
Seen so far: 25664 samples
Training loss (for one batch) at step 600 : 1.2826
Seen so far: 38464 samples
Training acc over epoch: 0.7194
Validation acc: 0.8192
Time taken: 1.08s

Start of epoch 1
Training loss (for one batch) at step 0 : 1.2518
Seen so far: 64 samples
Training loss (for one batch) at step 200 : 0.9677
Seen so far: 12864 samples
Training loss (for one batch) at step 400 : 0.5614
Seen so far: 25664 samples
Training loss (for one batch) at step 600 : 0.2599
Seen so far: 38464 samples
Training acc over epoch: 0.8374
Validation acc: 0.8660
Time taken: 0.76s
```

#### 4.2.3) Handling of losses tracked by the model

```python
Start of epoch 0
Training loss (for one batch) at step 0 : 1696.9138
Seen so far: 64 samples
Training loss (for one batch) at step 200 : 2.3027
Seen so far: 12864 samples
Training loss (for one batch) at step 400 : 2.3025
Seen so far: 25664 samples
Training loss (for one batch) at step 600 : 2.3024
Seen so far: 38464 samples
Training acc over epoch: 0.1129
Validation acc: 0.1063
Time taken: 1.18s

Start of epoch 1
Training loss (for one batch) at step 0 : 2.3030
Seen so far: 64 samples
Training loss (for one batch) at step 200 : 2.3032
Seen so far: 12864 samples
Training loss (for one batch) at step 400 : 2.3019
Seen so far: 25664 samples
Training loss (for one batch) at step 600 : 2.3029
Seen so far: 38464 samples
Training acc over epoch: 0.1136
Validation acc: 0.1063
Time taken: 0.68s
```

#### 4.2.4)  A GAN training loop from scratch

``` python
Model: "discriminator"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 14, 14, 64)        640       
_________________________________________________________________
leaky_re_lu (LeakyReLU)      (None, 14, 14, 64)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 7, 7, 128)         73856     
_________________________________________________________________
leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         
_________________________________________________________________
global_max_pooling2d (Global (None, 128)               0         
_________________________________________________________________
dense (Dense)                (None, 1)                 129       
=================================================================
Total params: 74,625
Trainable params: 74,625
Non-trainable params: 0
_________________________________________________________________
# 위의 출력은 discriminator.summary()를 주석처리 해제했을 때의 결과                      

Start epoch 0
discriminator loss at step 0: 0.69
adversarial loss at step 0: 0.68
```



### 4.3. 의미

` %.4f % (float(loss_value))` : 소숫점 4째자리까지 나타내기 위해 사용

1. *4.2.1*과 *4.2.2*를 비교하여 tf.function을 사용하였을때 시간이 많이 단축된 것을 확인할 수 있었습니다.
   따라서 코드를 짜면서 *4.1.1*과 같이 for문안에서 with 또는 다른 연산을 수행하는 것보다 *4.2.2*와 같이 tf.function을 사용하여 코드를 짜는 편이 training time은 줄이고 비슷한 결과를 얻을 수 있으므로 tf.function을 사용하는 것이 좋습니다.



2. *4.2.2*의 결과와 *4.2.3*의 결과를 비교해 보면 training loss의 값의 차이를 알 수 있습니다.

   "Layers & models recursively track any losses created during the forward pass by layers that call `self.add_loss(value)`. The resulting list of scalar loss values are available via the property `model.losses` at the end of the forward pass."

   "계층 및 모델은 "self.add_loss(값)"라고 하는 계층별 전달 패스 중에 생성된 손실을 재귀적으로 추적한다. 스칼라 손실 값의 결과 목록은 포워드 패스의 끝에 있는 속성 'model.loss'를 통해 이용할 수 있다."

   즉 순방향으로 layers를 지나며 가중치 같은 값들도 손실이 잃어나 결과적으로 중요한 가중치를 잃어버릴 수 있으니 이러한 부분의 손실을 막기 위하여 손실된 값들(loss)을 어느정도 추적하여 필요한 값들을 잃지않게 해주는, 다시 더해주는 작업이라고 이해하였습니다.





## 5. tf_function.py

### 5. 코드 및 결과

#### 5.1) Setup

``` python
# -*- coding: utf-8 -*-

# Some helper code to demonstrate the kinds of errors you might encounter.
# 에러 출력을 위한 헬퍼 함수
@contextlib.contextmanager
def assert_raises(error_class):
  try:
    yield
  except error_class as e:
    print('Caught expected exception \n  {}:'.format(error_class))
    # 기대하는 예외 발생
    traceback.print_exc(limit=2)
  except Exception as e:
    raise e
  else:
    raise Exception('Expected {} to be raised but no error was raised!'.format(
        error_class))
    # {}를 기대했지만 아무런 에러도 발생되지 않았습니다!
    
    
------------------------------------------------------------

Process finished with exit code 0
```

- 5.1) 에러 출력을 위한 헬퍼 함수를 정의합니다.



#### 5.2) Basics

``` python
import tensorflow as tf
import timeit

@tf.function  # The decorator converts `add` into a `Function`.
def add(a, b):
  return a + b

ans1 = add(tf.ones([2, 2]), tf.ones([2, 2]))
print("ans1 : ", ans1)

v = tf.Variable(1.0)
with tf.GradientTape() as tape:
  result = add(v, 1.0)

ans2 = tape.gradient(result, v)
print("ans2 : ", ans2)

@tf.function
def dense_layer(x, w, b):
  return add(tf.matmul(x, w), b)

ans3 = dense_layer(tf.ones([3, 2]), tf.ones([2, 2]), tf.ones([2]))
print("ans3 : ", ans3)


conv_layer = tf.keras.layers.Conv2D(100, 3)

@tf.function
def conv_fn(image):
  return conv_layer(image)

image = tf.zeros([1, 200, 200, 100])
# warm up
conv_layer(image); conv_fn(image)
print("Eager conv:", timeit.timeit(lambda: conv_layer(image), number=10))
print("Function conv:", timeit.timeit(lambda: conv_fn(image), number=10))
print("Note how there's not much difference in performance for convolutions")
```

실행 결과는 다음과 같습니다.

``` python
ans1 :  tf.Tensor(
[[2. 2.]
 [2. 2.]], shape=(2, 2), dtype=float32)
ans2 :  tf.Tensor(1.0, shape=(), dtype=float32)
ans3 :  tf.Tensor(
[[3. 3.]
 [3. 3.]
 [3. 3.]], shape=(3, 2), dtype=float32)
Eager conv: 0.0022273069989751093
Function conv: 0.003183538996381685
Note how there's not much difference in performance for convolutions
```

- 5.2) `@tf.function`deecorator으로 정의한 함수는 기본 텐서플로 연산과 같으며 Context Manager 및 다른 함수의 내부에서도 사용이 가능합니다.
  즉시 실행되는 eager모드보다 빠른 결과를 보여줍니다. 다만 계산량이 많은 연산들은 속도 향상이 크지 않습니다.



#### 5.3) Tracing

``` python
import tensorflow as tf

@tf.function
def double(a):
  print("Tracing with", a)
  return a + a

print(double(tf.constant(1)))
print()
print(double(tf.constant(1.1)))
print()
print(double(tf.constant("a")))
print()

# 결과에서 Tracing with Tensor .... 이 안뜨는 이유 :
# 동일한 인수유형으로 반복적으로 호출하면 생성된 그래프가 동일하여 TensorFlow가 추적단계를 건너뛰고 이전에 추적 한 그래프를 재사용하기 때문이다.
print(double(tf.constant("b")), "\n")

# pretty_printed_concrete_signatures()을 사용하여 사용 가능한 모든 추적을 볼 수 있다 .
print(double.pretty_printed_concrete_signatures())
```

실행 결과는 다음과 같습니다.

``` python
Tracing with Tensor("a:0", shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)

Tracing with Tensor("a:0", shape=(), dtype=float32)
tf.Tensor(2.2, shape=(), dtype=float32)

Tracing with Tensor("a:0", shape=(), dtype=string)
tf.Tensor(b'aa', shape=(), dtype=string)

tf.Tensor(b'bb', shape=(), dtype=string) 

double(a)
  Args:
    a: float32 Tensor, shape=()
  Returns:
    float32 Tensor, shape=()

double(a)
  Args:
    a: int32 Tensor, shape=()
  Returns:
    int32 Tensor, shape=()

double(a)
  Args:
    a: string Tensor, shape=()
  Returns:
    string Tensor, shape=()
```

- 5.1.3) 동일한 인수유형으로 반복적으로 호출하면 생성된 그래프가 동일하여 TensorFlow가 추적단계를 건너뛰고 이전에 추적 한 그래프를 재사용한다. (두번째의 b를 넣었을 때의 결과에서 Tracing with Tensor .... 이 안뜨는 이유) 
  Tracing은 `tf.Graph`와 `tf.Graph`를 감싸는 `ConcreteFunction`를 생성합니다. trace라고도 합니다.



#### 5.4) Controlling retracing

##### 5.4.1) Specify `input_signature` in `tf.function` to limit tracing.

``` python
import tensorflow as tf
import traceback
import contextlib

# Some helper code to demonstrate the kinds of errors you might encounter.
# 에러 출력을 위한 헬퍼 함수
@contextlib.contextmanager
def assert_raises(error_class):
  try:
    yield
  except error_class as e:
    print('Caught expected exception \n  {}:'.format(error_class))
    # 기대하는 예외 발생
    traceback.print_exc(limit=2)
  except Exception as e:
    raise e
  else:
    raise Exception('Expected {} to be raised but no error was raised!'.format(
        error_class))
    # {}를 기대했지만 아무런 에러도 발생되지 않았습니다!

@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))
def next_collatz(x):
  print("Tracing with", x)
  return tf.where(x % 2 == 0, x // 2, 3 * x + 1)

print(next_collatz(tf.constant([1, 2])))
# We specified a 1-D tensor in the input signature, so this should fail.
with assert_raises(ValueError):
  next_collatz(tf.constant([[1, 2], [3, 4]]))

# We specified an int32 dtype in the input signature, so this should fail.
with assert_raises(ValueError):
  next_collatz(tf.constant([1.0, 2.0]))
```

실행 결과는 다음과 같습니다.

``` python
Tracing with Tensor("x:0", shape=(None,), dtype=int32)
tf.Tensor([4 1], shape=(2,), dtype=int32)
Caught expected exception 
  <class 'ValueError'>:
Caught expected exception 
  <class 'ValueError'>:
Traceback (most recent call last):
  File "/home/kang/workspace/detlec/tf_function.py", line 13, in assert_raises
    yield
  File "/home/kang/workspace/detlec/tf_function.py", line 87, in <module>
    next_collatz(tf.constant([[1, 2], [3, 4]]))
ValueError: Python inputs incompatible with input_signature:
  inputs: (
    tf.Tensor(
[[1 2]
 [3 4]], shape=(2, 2), dtype=int32))
  input_signature: (
    TensorSpec(shape=(None,), dtype=tf.int32, name=None))
Traceback (most recent call last):
  File "/home/kang/workspace/detlec/tf_function.py", line 13, in assert_raises
    yield
  File "/home/kang/workspace/detlec/tf_function.py", line 91, in <module>
    next_collatz(tf.constant([1.0, 2.0]))
ValueError: Python inputs incompatible with input_signature:
  inputs: (
    tf.Tensor([1. 2.], shape=(2,), dtype=float32))
  input_signature: (
    TensorSpec(shape=(None,), dtype=tf.int32, name=None))
```

- 5.4.1) 추적을 제한하기 위하여 `tf.function`지정



##### 5.4.2) Specify a [None] dimension in `tf.TensorSpec` to allow for flexibility in trace reuse.

``` python
import tensorflow as tf

@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))
def g(x):
  print('Tracing with', x)
  return x

# No retrace!
print(g(tf.constant([1, 2, 3])))
print(g(tf.constant([1, 2, 3, 4, 5])))
```

실행 결과는 다음과 같습니다.

``` python
Tracing with Tensor("x:0", shape=(None,), dtype=int32)
tf.Tensor([1 2 3], shape=(3,), dtype=int32)
tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)
```

- 5.4.2) [None]차원에서 `tf.TensorSpec`은 추적을 사용하는데에 있어 유연성을 허용합니다.



##### 5.4.3) Cast Python arguments to Tensors to reduce retracing.

``` python
import tensorflow as tf

def train_one_step():
  pass

@tf.function
def train(num_steps):
  print("Tracing with num_steps = ", num_steps)
  tf.print("Executing with num_steps = ", num_steps)
  for _ in tf.range(num_steps):
    train_one_step()

print("Retracing occurs for different Python arguments.")
train(num_steps=10)
train(num_steps=20)

print()
print("Traces are reused for Tensor arguments.")
train(num_steps=tf.constant(10))
train(num_steps=tf.constant(20))

def f():
  print('Tracing!')
  tf.print('Executing')

tf.function(f)()
tf.function(f)()
```

실행 결과는 다음과 같습니다.

``` python
Retracing occurs for different Python arguments.
Tracing with num_steps =  10
Executing with num_steps =  10
Executing with num_steps =  20
Tracing with num_steps =  20

Traces are reused for Tensor arguments.
Tracing with num_steps =  Tensor("num_steps:0", shape=(), dtype=int32)
Executing with num_steps =  10
Executing with num_steps =  20
Tracing!
Tracing!
Executing
Executing
```



### 5.5) Obtaining concrete functions

- `get_concrete_function`을 사용하여 함수가 추적 될 때마다 새로운 구체적인 함수가 생성됩니다. 를 사용하여 구체적인 기능을 직접 얻을 수 있습니다.

```python
print("Obtaining concrete trace")
double_strings = double.get_concrete_function(tf.constant("a"))
print("Executing traced function")
print(double_strings(tf.constant("a")))
print(double_strings(a=tf.constant("b")))

# You can also call get_concrete_function on an InputSpec
double_strings_from_inputspec = double.get_concrete_function(tf.TensorSpec(shape=[], dtype=tf.string))
print(double_strings_from_inputspec(tf.constant("c")))

print(double_strings)

print(double_strings.structured_input_signature)
print(double_strings.structured_outputs)

with assert_raises(tf.errors.InvalidArgumentError):
  double_strings(tf.constant(1))

@tf.function
def pow(a, b):
  return a ** b

square = pow.get_concrete_function(a=tf.TensorSpec(None, tf.float32), b=2)
print(square)

assert square(tf.constant(10.0)) == 100

with assert_raises(TypeError):
  square(tf.constant(10.0), b=3)
```



### 5.6) Obtaining graphs

```python
graph = double_strings.graph
for node in graph.as_graph_def().node:
  print(f'{node.input} -> {node.name}')
```

실행한 결과는 다음과 같다.

```
[] -> a
['a', 'a'] -> add
['add'] -> Identity
```



### 5.8) AutoGraph Transformations

```python
# Simple loop
@tf.function
def f(x):
  while tf.reduce_sum(x) > 1:
    tf.print(x)
    x = tf.tanh(x)
  return x

f(tf.random.uniform([5]))

print(tf.autograph.to_code(f.python_function))
```

실행한 결과는 다음과 같다.

``` python
[0.208533764 0.729162335 0.785991788 0.791887403 0.854692817]
[0.205562651 0.622552633 0.65613246 0.659477 0.693513155]
[0.202715337 0.552902877 0.575783849 0.578015208 0.60023421]
[0.199983463 0.50269264 0.519594312 0.521221399 0.537216187]
[0.197359413 0.464232117 0.477386862 0.478642106 0.490877897]
[0.194836289 0.433527142 0.444148391 0.445155442 0.454912961]
[0.192407727 0.408264786 0.417077124 0.417908639 0.425929099]
[0.190067992 0.386998296 0.394465178 0.395167112 0.401913583]
[0.187811777 0.368769586 0.375203252 0.375806183 0.381585091]
[0.185634241 0.352915 0.358534515 0.35905987 0.364083201]
[0.183530897 0.338958144 0.343922526 0.344385654 0.348805577]
[0.181497633 0.326546937 0.330974758 0.331387073 0.335315824]
[0.179530591 0.315414637 0.319396347 0.319766551 0.323289096]
[0.177626282 0.305354923 0.308960944 0.309295803 0.312477887]
[0.175781459 0.296205401 0.299491495 0.299796343 0.302689642]
[0.173993051 0.287836164 0.290847182 0.291126251 0.293772042]
[0.172258243 0.280142 0.28291437 0.283171088 0.285602897]
[0.170574412 0.27303651 0.275600225 0.275837421 0.278082669]
[0.168939114 0.266448021 0.268828124 0.269048154 0.271129608]
[0.167350039 0.26031661 0.262534052 0.262738913 0.264675587]
[0.165805057 0.254591614 0.256664157 0.256855547 0.258663505]
[0.16430217 0.249229953 0.251172751 0.251352072 0.253045022]
[0.162839487 0.244194672 0.246020734 0.246189207 0.247778893]
[0.161415264 0.239453837 0.241174459 0.241333097 0.242829636]
[0.160027832 0.234979793 0.236604735 0.236754507 0.23816666]
[0.158675611 0.230748355 0.23228623 0.23242788 0.233763322]
[0.157357141 0.226738334 0.228196621 0.228330925 0.229596347]
[0.156071082 0.222931013 0.224316388 0.224443942 0.225645274]
[0.154816091 0.219309866 0.220628217 0.220749557 0.221892029]
[0.153590932 0.215860158 0.217116714 0.217232347 0.218320653]
[0.152394444 0.21256876 0.213768229 0.213878572 0.214916825]

def tf__f(x):
    with ag__.FunctionScope('f', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:
        do_return = False
        retval_ = ag__.UndefinedReturnValue()

        def get_state():
            return (x,)

        def set_state(vars_):
            nonlocal x
            (x,) = vars_

        def loop_body():
            nonlocal x
            ag__.converted_call(ag__.ld(tf).print, (ag__.ld(x),), None, fscope)
            x = ag__.converted_call(ag__.ld(tf).tanh, (ag__.ld(x),), None, fscope)

        def loop_test():
            return (ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.ld(x),), None, fscope) > 1)
        ag__.while_stmt(loop_test, loop_body, get_state, set_state, ('x',), {})
        try:
            do_return = True
            retval_ = ag__.ld(x)
        except:
            do_return = False
            raise
        return fscope.ret(retval_, do_return)
```



### 5.9) Conditionals

```python
@tf.function
def fizzbuzz(n):
  for i in tf.range(1, n + 1):
    print('Tracing for loop')
    if i % 15 == 0:
      print('Tracing fizzbuzz branch')
      tf.print('fizzbuzz')
    elif i % 3 == 0:
      print('Tracing fizz branch')
      tf.print('fizz')
    elif i % 5 == 0:
      print('Tracing buzz branch')
      tf.print('buzz')
    else:
      print('Tracing default branch')
      tf.print(i)

fizzbuzz(tf.constant(5))
fizzbuzz(tf.constant(20))
```

실행한 결과는 다음과 같다.

``` python
Tracing for loop
Tracing fizzbuzz branch
Tracing fizz branch
Tracing buzz branch
Tracing default branch
1
2
fizz
4
buzz
1
2
fizz
4
buzz
fizz
7
8
fizz
buzz
11
fizz
13
14
fizzbuzz
16
17
fizz
19
buzz
```



### 5.10) Loops

```python
def measure_graph_size(f, *args):
  g = f.get_concrete_function(*args).graph
  print("{}({}) contains {} nodes in its graph".format(
      f.__name__, ', '.join(map(str, args)), len(g.as_graph_def().node)))

@tf.function
def train(dataset):
  loss = tf.constant(0)
  for x, y in dataset:
    loss += tf.abs(y - x) # Some dummy computation.
  return loss

small_data = [(1, 1)] * 3
big_data = [(1, 1)] * 10
measure_graph_size(train, small_data)
measure_graph_size(train, big_data)

measure_graph_size(train, tf.data.Dataset.from_generator(
    lambda: small_data, (tf.int32, tf.int32)))
measure_graph_size(train, tf.data.Dataset.from_generator(
    lambda: big_data, (tf.int32, tf.int32)))
```

실행한 결과는 다음과 같다.

```python
train([(1, 1), (1, 1), (1, 1)]) contains 11 nodes in its graph
train([(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]) contains 32 nodes in its graph
train(<FlatMapDataset shapes: (<unknown>, <unknown>), types: (tf.int32, tf.int32)>) contains 10 nodes in its graph
train(<FlatMapDataset shapes: (<unknown>, <unknown>), types: (tf.int32, tf.int32)>) contains 10 nodes in its graph
```



#### 5.10.1) Accumulating values in a loop

```python
batch_size = 2
seq_len = 3
feature_size = 4

def rnn_step(inp, state):
  return inp + state

@tf.function
def dynamic_rnn(rnn_step, input_data, initial_state):
  # [batch, time, features] -> [time, batch, features]
  input_data = tf.transpose(input_data, [1, 0, 2])
  max_seq_len = input_data.shape[0]

  states = tf.TensorArray(tf.float32, size=max_seq_len)
  state = initial_state
  for i in tf.range(max_seq_len):
    state = rnn_step(input_data[i], state)
    states = states.write(i, state)
  return tf.transpose(states.stack(), [1, 0, 2])

dynamic_rnn(rnn_step,
            tf.random.uniform([batch_size, seq_len, feature_size]),
            tf.zeros([batch_size, feature_size]))
```



### 5.11) Limitations

```python
@tf.function
def f(x):
  print("Traced with", x)
  tf.print("Executed with", x)

f(1)
f(1)
f(2)

# ---
external_list = []

@tf.function
def side_effect(x):
  print('Python side effect')
  external_list.append(x)

side_effect(1)
side_effect(1)
side_effect(1)
# The list append only happened once!
assert len(external_list) == 1

# ---
@tf.function
def buggy_consume_next(iterator):
  tf.print("Value:", next(iterator))

iterator = iter([1, 2, 3])
buggy_consume_next(iterator)
# This reuses the first value from the iterator, rather than consuming the next value.
buggy_consume_next(iterator)
buggy_consume_next(iterator)

# ---
@tf.function
def good_consume_next(iterator):
  # This is ok, iterator is a tf.data.Iterator
  tf.print("Value:", next(iterator))

ds = tf.data.Dataset.from_tensor_slices([1, 2, 3])
iterator = iter(ds)
good_consume_next(iterator)
good_consume_next(iterator)
good_consume_next(iterator)
```



### 5.12) Deleting tf.Variables between `Function` calls

```python
external_var = tf.Variable(3)
@tf.function
def f(x):
  return x * external_var

traced_f = f.get_concrete_function(4)
print("Calling concrete function...")
print(traced_f(4))

# The original variable object gets garbage collected, since there are no more
# references to it.
external_var = tf.Variable(4)
print()
print("Calling concrete function after garbage collecting its closed Variable...")
with assert_raises(tf.errors.FailedPreconditionError):
  traced_f(4)
```



### 5.13) Depending on Python global and free variables

```python
@tf.function
def buggy_add():
  return 1 + foo

@tf.function
def recommended_add(foo):
  return 1 + foo

# ---
foo = 1
print("Buggy:", buggy_add())
print("Correct:", recommended_add(foo))

print("Updating the value of `foo` to 100!")
foo = 100
print("Buggy:", buggy_add())  # Did not change!
print("Correct:", recommended_add(foo))

# ---
class SimpleModel(tf.Module):
  def __init__(self):
    # These values are *not* tf.Variables.
    self.bias = 0.
    self.weight = 2.

@tf.function
def evaluate(model, x):
  return model.weight * x + model.bias

simple_model = SimpleModel()
x = tf.constant(10.)
print(evaluate(simple_model, x))

# ---
print("Adding bias!")
simple_model.bias += 5.0
print(evaluate(simple_model, x))  # Didn't change :(

# ---
def evaluate(model, x):
  return model.weight * x + model.bias

new_model = SimpleModel()
evaluate_no_bias = tf.function(evaluate).get_concrete_function(new_model, x)
# Don't pass in `new_model`, `Function` already captured its state during tracing.
print(evaluate_no_bias(x))

# ---
print("Adding bias!")
new_model.bias += 5.0
# Create new Function and ConcreteFunction since you modified new_model.
evaluate_with_bias = tf.function(evaluate).get_concrete_function(new_model, x)
print(evaluate_with_bias(x)) # Don't pass in `new_model`.

# ---
class BetterModel:

  def __init__(self):
    self.bias = tf.Variable(0.)
    self.weight = tf.Variable(2.)

@tf.function
def evaluate(model, x):
  return model.weight * x + model.bias

better_model = BetterModel()
print(evaluate(better_model, x))

# --- 
print("Adding bias!")
better_model.bias.assign_add(5.0)  # Note: instead of better_model.bias += 5
print(evaluate(better_model, x))  # This works!
```

---

