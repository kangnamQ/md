고급딥러닝(2021) HW1
==

### - 20217087	강남규





## 1. keras_classifier.py

### 1.1. 코드

``` python

```

### 1.2. 결과

``` python

```

### 1.3. 의미





## 2. adv_tf_classifier.py

### 2.1. 코드

``` python

```

### 2.2. 결과

``` python

```

### 2.3. 의미





## 3. torch_classifier.py

### 3.1. 코드

``` python

```

### 3.2. 결과

``` python

```

### 3.3. 의미





## 4. training_loop.py

### 4.1. 코드

##### 4.1.1) Low-level handling of metrics

``` python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import time

inputs = keras.Input(shape=(784,), name="digits")
x = layers.Dense(64, activation="relu", name="dense_1")(inputs)
x = layers.Dense(64, activation="relu", name="dense_2")(x)
outputs = layers.Dense(10, name="predictions")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

optimizer = keras.optimizers.SGD(learning_rate=1e-3)

loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

batch_size = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = np.reshape(x_train, (-1, 784))
x_test = np.reshape(x_test, (-1, 784))

x_val = x_train[-10000:]
y_val = y_train[-10000:]
x_train = x_train[:-10000]
y_train = y_train[:-10000]

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_dataset = val_dataset.batch(batch_size)

epochs = 2
for epoch in range(epochs):
    print(f"\nStart of epoch {epoch}")
    start_time = time.time()

    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        with tf.GradientTape() as tape:
            logits = model(x_batch_train, training=True)
            loss_value = loss_fn(y_batch_train, logits)
        grads = tape.gradient(loss_value, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))

        train_acc_metric.update_state(y_batch_train, logits)

        if step % 200 == 0:
            print(f"Training loss (for one batch) at step {step} : %.4f" % (float(loss_value)))
            print(f"Seen so far: {((step + 1) * batch_size)} samples")

    train_acc = train_acc_metric.result()
    print(f"Training acc over epoch: %.4f" % float(train_acc))

    train_acc_metric.reset_states()

    for x_batch_val, y_batch_val in val_dataset:
        val_logits = model(x_batch_val, training=False)
        val_acc_metric.update_state(y_batch_val, val_logits)
    val_acc = val_acc_metric.result()
    val_acc_metric.reset_states()
    print(f"Validation acc: %.4f" % float(val_acc))
    print(f"Time taken: %.2fs" % (time.time() - start_time))
```



### 4.2. 결과

#### 4.2.1) Low-level handling of metrics

``` python
Start of epoch 0
Training loss (for one batch) at step 0 : 83.1320
Seen so far: 64 samples
Training loss (for one batch) at step 200 : 1.5567
Seen so far: 12864 samples
Training loss (for one batch) at step 400 : 1.4023
Seen so far: 25664 samples
Training loss (for one batch) at step 600 : 1.1630
Seen so far: 38464 samples
Training acc over epoch: 0.6603
Validation acc: 0.7799
Time taken: 3.73s

Start of epoch 1
Training loss (for one batch) at step 0 : 0.8439
Seen so far: 64 samples
Training loss (for one batch) at step 200 : 0.5479
Seen so far: 12864 samples
Training loss (for one batch) at step 400 : 0.7670
Seen so far: 25664 samples
Training loss (for one batch) at step 600 : 0.7305
Seen so far: 38464 samples
Training acc over epoch: 0.8071
Validation acc: 0.8452
Time taken: 3.49s
```

### 4.3. 의미

` %.4f % (float(loss_value))` : 소숫점 4째자리까지 나타내기 위해 사용

``` python

```





## 5. tf_function.py

### 5.1. 코드

``` python

```

### 5.2. 결과

``` python

```

### 5.3. 의미