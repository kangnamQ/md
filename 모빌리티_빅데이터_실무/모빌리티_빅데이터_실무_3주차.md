모빌리티 빅데이터 실무
===

세부정보
---

- 담당 : 최혁두 교수님
- 일시 : 화  14:00 ~ 17:00
- 주 사용 플랫폼 : Flow, Zoom, github.io
- 주차 : 3주차 - 21.03.16

---

참조
---

Reference : [IanLecture][IanLecture_link] (교수님 블로그)

[IanLecture_link]: https://goodgodgd.github.io/ian-lecture/archivers/dt-classifier "IanLecture_Reference"

---

Image Classifiers
===

- 목적 : 오브젝트 디텍터(Object Detector)를 구현하기 위함



Implement Image Classifiers
==

영상 분류 모델을 생성, 학습, 평가 코드 구현을 텐서플로와 파이토치 두 가지 프레임워크에서 배워본다.



## 1.3. Advanced Model

Keras를 활용하면 편리한 점도 있지만 학습 과정이 `model.fit()`이란 함수 안에서 일어나기 때문에 학습이 잘못됐을 때 어디서 무엇이 잘못됐는지 알기 어렵다.
학습 과정 중에 중간 결과물을 저장하고 싶거나 체크포인트를 저장하고 싶으면 `model.fit()`함수에 callback 객체들을 넣으면 되긴 한다.
하지만 여러가지 callback 객체의 사용법을 익히는 것도 번거롭고 딱 내가 원하는대로 하기 어려운 경우도 있다.
Keras에서 주어지는 loss나 metric도 검출 모델을 학습하는데는 적합하지 않기 때문에 어차피 따로 만들어줘야 한다.

다음은 관련된 Keras 링크다.

- https://keras.io/api/callbacks/
- https://keras.io/api/losses/
- https://keras.io/api/metrics/

그러므로 검출 모델과 같이 복잡한 모델을 구현할 때는 Keras API는 모델을 정의하는 레이어 객체를 만드는 정도에만 사용하고 loss나 metric 등의 계산은 텐서플로의 함수들을 사용한다. 검출기와 같이 복잡한 모델을 학습하기 위해서는 아래와 같은 텐서플로의 고급 사용법들을 익혀야 한다.

1. `Keras functional API` : 일렬 연결이 아닌 복잡한 구조의 모델 정의
2. `tf.GradientTape` : loss에 대한 미분을 명시적으로 계산하여 모델 weight 업데이트
3. `tf.function` : 모델을 eager 모드가 아닌 정적 그래프에서 실행하여 학습 속도 증가
4. `tf.data.Dataset` : 파일이나 메모리에서 데이터를 불러와 반복 가능한 객체로 만들어 학습 과정에 데이터 주입.



### a) 코드 구조

코드의 전체적인 흐름은 Keras Classifier와 유사하다. 
하지만 이번에는 `AdvancedClassifier`라는 클래스를 만들어서 분류 모델에 관련된 코드들을 응집시켰다. 
Common Utils 아래의 코드는 위와 동일하다.
또 한가지 차이점은 `@tf.function` 데코레이터를 사용했다는 것이다.
자세한 내용은 아래 내용을 참고한다.

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
from timeit import default_timer as timer

"""
Common utils
"""
class DurationTime:
    pass

def load_dataset(dataname="cifar10", show_imgs=True):
    pass

def show_samples(images, labels, class_names, grid=(3,4)):
    pass

"""
Classifier
"""
class AdvancedClassifier:
    def __init__(self, batch_size=32, val_ratio=0.2):
        pass
    
    def build_model(self, x, y):
        pass
    
    def train(self, x, y, epochs, eager_mode=True):
        pass
    
    @tf.function
    def train_batch_graph(self, x_batch, y_batch):
        pass

    def evaluate(self, x, y_true, verbose=True):
        pass

def tf2_advanced_classifier():
    gpu_config()
    (x_train, y_train), (x_test, y_test) = load_dataset("cifar10")
    clsf = AdvancedClassifier()
    clsf.build_model(x_train, y_train)
    clsf.train(x_train, y_train, 5)
    clsf.evaluate(x_test, y_test)

if __name__ == "__main__":
    tf2_advanced_classifier()
```



### b) 클래스 초기화

생성자 함수에서는 학습에 필요한 객체를 생선하고 설정 값들을 저장한다.
모델은 이후 `build_model()`이란 함수에서 정의할 것이지만 파이참의 자동완성을 위해 생성자에서 기본 객체를 만들어둔다.
Loss와 optimizer 객체는 이전 코드와 동일하게 선택하였다.

``` python
    def __init__(self, batch_size=32, val_ratio=0.2):
        self.model = keras.Model()
        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
        self.optimizer = tf.keras.optimizers.RMSprop()
        self.batch_size = batch_size
        self.val_ratio = val_ratio
```



### c) 모델 정의 (Functional API)

여기서는 모델을 순차 모델이 아닌 Funtional API를 이용해 구현한다.
Funtional API는 각 레이어의 입력과 출력을 명시적으로 지정하므로 어느 레이어의 출력이 어느 레이어의 입력이 되는지 명확히 볼 수 있다.
또한 레이어 사이의 연결이 자유로워져서 Inception모듈처럼 여러 갈래로 갈라지는 모델이나 출력이 여러개인 모델도 만들 수 있다.

여기서 레이어 객체들을 마치 함수처럼 쓰는 것을 볼 수 있는데 이런 것을 callable 객체, 혹은 functor라고도 한다.
클래스 내부에 `__call__()` 함수가 정의되어 있으면 함수 이름없이 바로 `__call__()`의 입력인자를 넣으면 `__call__()`이 실행된다.
텐서플로에는 어떤 클래스에서 외부에서 사용되는 함수가 하나뿐인 경우 이렇게 callable을 사용하는 경우가 많다.

보통 Class를 만들 때 안에서 작동하는 함수가 하나인 편이 좋기는 하다.
`__call__()`함수가 있다면 클래스 이름을 적고 넣어야하는 인자를 넣으면 곧바로 실행되는 방법이다.



`tf.keras.Model`클래스도 `model.predict(x)`함수를 이용해 출력을 계산할 수 있지만 `model(x)`도 동일하게 작동한다.
이는 객체지향의 단일 책임 원칙(SRP, Single Responsibility Principle)에도 부합하며 클래스의 의도를 명확하게 보여준다.

레이어를 다 연결하고 나서 `tf.keras.Model`클래스를 이용해 모델의 입력과 출력 텐서를 지정하면 그 사이의 연산 그래프가 모델 내부로 들어가서 특정한 연산을 하는 모델이 정의된다.

``` python
  def build_model(self, x, y):
        input_shape = x.shape[1:]
        num_class = tf.reduce_max(y).numpy() + 1
        input_tensor = layers.Input(shape=input_shape)
        x = layers.Conv2D(filters=32, kernel_size=3, padding="same", activation="relu", name="conv1")(input_tensor)
        x = layers.MaxPool2D(pool_size=(2, 2), name="pooling1")(x)
        x = layers.Conv2D(filters=64, kernel_size=3, padding="same", activation="relu", name="conv2")(x)
        x = layers.MaxPool2D(pool_size=(2, 2), name="pooling2")(x)
        x = layers.Flatten(name="flatten")(x)
        x = layers.Dense(units=100, activation="relu", name="dense1")(x)
        x = layers.Dropout(0.2)(x)
        output_tensor = layers.Dense(units=num_class, activation="softmax", name="dense2")(x)
        self.model = keras.Model(inputs=input_tensor, outputs=output_tensor, name="tf-classifier")
        self.model.summary()
        keras.utils.plot_model(self.model, "tf-clsf-model-adv.png")
```

실행 결과는 이전 결과와 같다.

```python
Model: "tf-classifier"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 32, 32, 32)        896       
_________________________________________________________________
pooling1 (MaxPooling2D)      (None, 16, 16, 32)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 16, 16, 64)        18496     
_________________________________________________________________
pooling2 (MaxPooling2D)      (None, 8, 8, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 4096)              0         
_________________________________________________________________
dense1 (Dense)               (None, 100)               409700    
_________________________________________________________________
dropout (Dropout)            (None, 100)               0         
_________________________________________________________________
dense2 (Dense)               (None, 10)                1010      
=================================================================
Total params: 430,102
Trainable params: 430,102
Non-trainable params: 0
_________________________________________________________________
```



### d) 학습

이전 코드에서는 `model.fit()`만 사용하면 학습이 되었지만 학습 과정을 자세히 들여다보긴 어려웠다.
Loss 함수가 복잡해지고 학습 과정을 디버깅해야 한다면 `tf.GradientTape()`을 이용해 학습 과정을 직접 프로그래밍 해주는 것이 나을 것이다.
아래 `train()`함수에서는 주로 학습 전후의 데이터 준비 및 성능 평가를 하고 실제 학습은 `train_batch_graph()`함수에서 일어난다.

```python
    def train(self, x, y, epochs):
        trainlen = int(x.shape[0] * (1 - self.val_ratio))
        x_train, y_train = x[:trainlen], y[:trainlen]
        x_val, y_val = x[trainlen:], y[trainlen:]

        dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
        dataset = dataset.shuffle(200).batch(self.batch_size)
        with DurationTime("** training time") as duration:
            for epoch in range(epochs):
                for x_batch, y_batch in dataset:
                    self.train_batch_graph(x_batch, y_batch)
                loss, accuracy = self.evaluate(x_val, y_val, verbose=False)
                print(f"[Training] epoch={epoch}, val_loss={loss}, val_accuracy={accuracy}")
```

실행결과는 다음과 같다.

```python
[Training] epoch=0, val_loss=1.2970373630523682, val_accuracy=0.5541
[Training] epoch=1, val_loss=1.0697484016418457, val_accuracy=0.6177
[Training] epoch=2, val_loss=0.9729103446006775, val_accuracy=0.6611
[Training] epoch=3, val_loss=0.9531869292259216, val_accuracy=0.6713
[Training] epoch=4, val_loss=0.9794153571128845, val_accuracy=0.6702
** training time: 17.34
```



#### `tf.data.Dataset`

`train()`함수에서 눈여겨 봐야 할 것은 `tf.data.Dataset`객체다.
`Dataset`은 텐서플로에서 데이터 입력 파이프라인을 최적화 시켜주는 클래스다. 다양한 형태의 입력 데이터를 일정한 단위로 뽑아서 사용할 수 있게 해주고 다양한 전처리를 적용할 수 있다.

메모리에 올려진 List나 Numpy 데이터를 사용할 경우 `from_tensor_slices()`함수를 이용한다.
리턴된 `dataset`객체는 for문에서 반복이 가능하다. 그리고 `shuffle()`이나 `batch()`함수를 통해 데이터를 섞거나 배치 단위로 묶을 수 있다. 
위 코드에서는 학습 데이터의 순서를 섞고 `self.batch_size` (32개) 단위로 데이터를 묶어서 보낸다.
학습은 5 에폭을 반복하는데 이것도 사실 `repeat(5) `함수까지 붙이면 `dataset`객체가 한번의 for문에서 5 에폭을 반복할 수 있다.
하지만 1에폭이 끝날때마다 성능을 확인하기 위해 사용하지 않았다.

Dataset에 대한 자세한 내용은 아래 링크에서 볼 수 있고 다음 강의에서 더 자세히 다룬다고 하셨다.
https://www.tensorflow.org/guide/data



#### `tf.GradientTape`

실제 학습 함수는 다음과 같다.

``` python
 @tf.function
    def train_batch_graph(self, x_batch, y_batch):
        with tf.GradientTape() as tape:
            # training=True is only needed if there are layers with different
            # behavior during training versus inference (e.g. Dropout).
            predictions = self.model(x_batch, training=True)
            loss = self.loss_object(y_batch, predictions)
        gradients = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
```

위 함수에서는 `tf.GradientTape`을 이용해 학습을 진행한다. 
GradientTape이란 말처럼 이는 해당 context 아래서 발생하는 모든 연산의 미분값을 기록한다.
여기서는 모델의 입력에서 출력이 나오고 출력으로부터 손실 함수를 계산하는 것까지 context에서 계산하였다.
왜냐하면 loss값을 모델의 파라미터(weights)에 대해서 미분해야하기 때문인다.
`tape.gradient()` 함수에 미분의 분자, 분모 변수를 지정하면 미분값들을 가져올 수 있다.
그리고 이 미분값들을 optimizer에 적용하면 모델의 파라미터가 업데이트 된다.

GradientTape에 대한 자세한 내용은 다음 링크를 참조한다.

- https://www.tensorflow.org/guide/autodiff
- https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit
- https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch



#### `tf.function`

텐서플로의 기본 모드인 eager 모드는 마치 Numpy 연산을 하듯 모든 라인을 한 줄씩 파이썬 인터프리터에서 실행하고 모든 중간 결과를 확인할 수 있다.
반면 어떤 함수에 `@tf.function` 데코레이터가 붙으면..

- 해당 함수와 그 아래에서 불러지는 모든 함수에서 실행되는 모든 텐서 연산들이 텐서플로 내부적으로 최적화된다. 연산 과정 자체가 더 빠르게 실행될 수 있도록 변한다.
- 매 연산마다 그때 그때 메모리를 준비하는 것이 아니라 전체 연산에 필요한 모든 메모리를 미리 준비해서 정적 그래프를 만들어놓고 입력 데이터가 연산 그래프를 따라 흘러가게 한다.
- GPU 연산을 하는 경우 eager 모드에서는 매 연산마다 메인 메모리(RAM)에서 GPU로 데이터를 보내고 결과를 다시 받아와야 하지만 graph 모드에서는 연산을 모두 마친 후에 최종 결과만 받는다.

작은 모델에서는 graph 모드가 효과가 없거나 더 느려질수도 있지만 복잡한 모델에서는 graph 모드의 속도가 훨씬 빨라질 수 있다. 
교수님은 모델마다 다르지만 개인적인 경험으로 많이 사용하는 CNN에서 2배 이상 빨라지는 것을 경험하셨다고 한다.
하지만 현재 예제에서는 모델이 작기 때문에 오히려 속도가 느려지는 것을 확인하셨다.
그리고 `Dataset`객체를 만드는 과정까지 graph모드에 넣으면 추가적인 최적화를 할 수 있다.
참고로 `model.fit()`함수에서는 모든 최적화가 이미 적용되어 있다.

다음은 graph모드와 관련된 링크다.

- https://www.tensorflow.org/guide/intro_to_graphs
- https://www.tensorflow.org/guide/function
- https://www.tensorflow.org/guide/graph_optimization



#### `Shape, Eager, Graph`

텐서플로에는 Tensor의 shape을 확인하는 세 가지 방법이 있는데
1,2번은 사실상 동일하고 3번은 타입이 다르다.

1. `some_tensor.shape` (tf.TensorShape)
2. `some_tensor.get_shape()` (tf.TensorShape)
3. `tf.shape(some_tensor)` (tf.Tensor)

Eager 모드에서는 어떤 것을 써도 상관 없지만 Graph모드에서는 다르다.

``` python
import tensorflow as tf

def main():
    # create Tensor from List with specific type
    x = tf.constant([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]], dtype=tf.int32)
    shape_out = print_tensor_shape(x, "eager")
    print("eager 6) shape output:", shape_out)
    shape_out = print_tensor_shape_graph(x, "graph")
    print("graph 6) shape output:", shape_out)
    shape_out = print_tensor_shape_graph(x, "graph")

def print_tensor_shape(tensor, title):
    print(f"{title} 1) Tensor.shape:", tensor.shape, type(tensor.shape))
    print(f"{title} 2) Tensor.get_shape():", tensor.get_shape())
    print(f"{title} 3) tf.shape():", tf.shape(tensor))
    h, w = tensor[0, 0, 1], tensor[0, 0, 2]
    zeros = tf.zeros((h, w))
    print(f"{title} 4) Tensor.shape:", zeros.shape)
    print(f"{title} 5) tf.shape():", tf.shape(zeros))
    return tf.shape(zeros)

@tf.function
def print_tensor_shape_graph(tensor, title):
    return print_tensor_shape(tensor, title)

if __name__ == "__main__":
    main()
```

실행결과는 다음과 같다.

```python
eager 1) Tensor.shape: (2, 2, 3) <class 'tensorflow.python.framework.tensor_shape.TensorShape'>
eager 2) Tensor.get_shape(): (2, 2, 3)
eager 3) tf.shape(): tf.Tensor([2 2 3], shape=(3,), dtype=int32)
eager 4) Tensor.shape: (2, 3)
eager 5) tf.shape(): tf.Tensor([2 3], shape=(2,), dtype=int32)
eager 6) shape output: tf.Tensor([2 3], shape=(2,), dtype=int32)

graph 1) Tensor.shape: (2, 2, 3) <class 'tensorflow.python.framework.tensor_shape.TensorShape'>
graph 2) Tensor.get_shape(): (2, 2, 3)
graph 3) tf.shape(): Tensor("Shape:0", shape=(3,), dtype=int32)
graph 4) Tensor.shape: (None, None)
graph 5) tf.shape(): Tensor("Shape_1:0", shape=(2,), dtype=int32)
graph 6) shape output: tf.Tensor([2 3], shape=(2,), dtype=int32)
```

Eager 모드에서는 일반 파이썬 프로그램처럼 한 줄씩 실행하므로 객체 생성과 동시에 연산을 수행한다.
그래서 텐서값에 의해 shape가 결정되도 바로 shape을 확인할 수 있다.

반면 graph모드에서는 미리 연산을 수행할 객체들을 미리 준비한 다음 연산을 실행하는데 `print()`가 실행되는 시점은 객체를 준비하는 시점이라서 텐서의 구체적인 값이 정해져 있지 않다.
그래서 shape가 None으로 나온다.(`print_tensor_shape_graph`을 두번 실행하지만 출력은 한번씩 밖에 되지 않는다.)

`Tensor.shape`이나 `tf.shape()`이나 결과적으로는 큰 차이가 없는데 `print()`에서 차이가 난다.
그리고 `@tf.function`함수의 출력은 오직 `tf.Tensor`타입만 가능하므로 `tf.TensorShape`타입은 출력으로 사용되지 못한다.

- `Tensor.get_shape()`: https://www.tensorflow.org/api_docs/python/tf/Tensor#get_shape
- `tf.shape()` : https://www.tensorflow.org/api_docs/python/tf/shape



### e) 평가

성능 평가 함수는 이전과 크게 다르지 않다.
여기서는 모델의 출력을 `model.predict()`함수가 아닌 `model.__call__()`함수를 이용하여 객체를 직접 불렀다.
Loss와 accuracy를 계산하여 리턴한다.

```python
    def evaluate(self, x, y_true, verbose=True):
        if verbose:
            print("[evaluate] predict by model.__call__()")
        y_pred = self.model(x)
        accuracy = np.mean(np.argmax(y_pred, axis=1) == y_true)
        loss = self.loss_object(y_true, y_pred)
        if verbose:
            np.set_printoptions(precision=4, suppress=True)
            print("  prediction shape:", y_pred.shape, y_true.shape)
            print("  first 5 predicts:\n", y_pred[:5].numpy())
            print("  check probability:", np.sum(y_pred[:5], axis=1))
            print(f"  loss={loss:1.4f}, accuracy={accuracy:1.4f}")
        return loss, accuracy
```

실행결과는 다음과 같다.

``` python
[evaluate] predict by model.__call__()
  prediction shape: (10000, 10) (10000, 1)
  first 5 predicts:
 [[0.     0.     0.     0.7727 0.     0.2271 0.0001 0.     0.     0.    ]
 [0.0048 0.0774 0.     0.     0.     0.     0.     0.     0.9147 0.0032]
 [0.2223 0.1069 0.0066 0.0323 0.0007 0.0035 0.0003 0.0006 0.5874 0.0393]
 [0.7653 0.0052 0.0941 0.0004 0.0076 0.     0.     0.     0.1274 0.0001]
 [0.     0.0009 0.0188 0.185  0.3672 0.0141 0.4137 0.     0.0001 0.0001]]
  check probability: [1. 1. 1. 1. 1.]
  loss and accuracy: tf.Tensor(0.9907759, shape=(), dtype=float32) 0.6779
```

----

